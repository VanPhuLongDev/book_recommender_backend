# -*- coding: utf-8 -*-
"""full_convmf_vn_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/full-convmf-vn-2-b8f4b0e5-73f9-4682-9ad6-d4025d76cb15.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240612/auto/storage/goog4_request%26X-Goog-Date%3D20240612T175244Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8930e2e41aca364d76c5d9b7434413d7228b2b6190f954acd2f6363ffb117c5c2274a9438b426e27c5ee39fb2a5d0333ce02fa62bcf43446c53bb401f72638f78515f07270095294f74e81914d627f5165d8236c075a1c8fbe8859359766bd134fab1ba4220a013377d112742cb8221d7c873ca599d56900c90e42efd3e08ab3c964b31580417edb6155fede8cf6ea2c221b30a498d72fd50c4161200399e1e972ba16e11befa0216ccd5577e992adc72af3c72efda3aa1c160dc746ca55389cf7dac9cac0a145bbe619cf812636f6456d9b399742218b563f974099922c6ea7665ea35e704abcb104f23e0be1dbb98ee13174279ee7d30385dc0166eb94ad86
"""

# !pip install ipdb
# %pdb on

# !pip install pyvi

import ast
import itertools
import math
from collections import Counter
from tqdm.auto import trange
import logging


def ui_parser(tokens, line_idx, id_inline=False, **kwargs):
    if id_inline:
        return [(str(line_idx + 1), iid, 1.0) for iid in tokens]
    else:
        return [(tokens[0], iid, 1.0) for iid in tokens[1:]]


def uir_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], float(tokens[2]))]


def review_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], tokens[2])]


def uirt_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], float(tokens[2]), int(tokens[3]))]


def tup_parser(tokens, **kwargs):
    return [
        (
            tokens[0],
            tokens[1],
            [tuple(tup.split(kwargs.get("tup_sep"))) for tup in tokens[2:]],
        )
    ]


def ubi_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], tokens[2])]


def ubit_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], tokens[2], int(tokens[3]))]


def ubitjson_parser(tokens, **kwargs):
    return [
        (tokens[0], tokens[1], tokens[2], int(tokens[3]), ast.literal_eval(tokens[4]))
    ]


def sit_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], int(tokens[2]))]


def sitjson_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], int(tokens[2]), ast.literal_eval(tokens[3]))]


def usit_parser(tokens, **kwargs):
    return [(tokens[0], tokens[1], tokens[2], int(tokens[3]))]


def usitjson_parser(tokens, **kwargs):
    return [
        (tokens[0], tokens[1], tokens[2], int(tokens[3]), ast.literal_eval(tokens[4]))
    ]


PARSERS = {
    "UI": ui_parser,
    "UIR": uir_parser,
    "UIRT": uirt_parser,
    "UITup": tup_parser,
    "UIReview": review_parser,
    "UBI": ubi_parser,
    "UBIT": ubit_parser,
    "UBITJson": ubitjson_parser,
    "SIT": sit_parser,
    "SITJson": sitjson_parser,
    "USIT": usit_parser,
    "USITJson": usitjson_parser,
}


class Reader:
    """Reader class for reading data with different types of format.

    Parameters
    ----------
    user_set: set, default = None
        Set of users to be retained when reading data.
        If `None`, all users will be included.

    item_set: set, default = None
        Set of items to be retained when reading data.
        If `None`, all items will be included.

    min_user_freq: int, default = 1
        The minimum frequency of a user to be retained.
        If `min_user_freq = 1`, all users will be included.

    min_item_freq: int, default = 1
        The minimum frequency of an item to be retained.
        If `min_item_freq = 1`, all items will be included.

    num_top_freq_user: int, default = 0
        The number of top popular users to be retained.
        If `num_top_freq_user = 0`, all users will be included.

    num_top_freq_item: int, default = 0
        The number of top popular items to be retained.
        If `num_top_freq_item = 0`, all items will be included.

    min_basket_size: int, default = 1
        The minimum number of items of a basket to be retained.
        If `min_basket_size = 1`, all items will be included.

    max_basket_size: int, default = -1
        The maximum number of items of a basket to be retained.
        If `min_basket_size = -1`, all items will be included.

    min_basket_sequence: int, default = 1
        The minimum number of baskets of a user to be retained.
        If `min_basket_sequence = 1`, all baskets will be included.

    min_sequence_size: int, default = 1
        The minimum number of items of a sequence to be retained.
        If `min_sequence_size = 1`, all sequences will be included.

    max_sequence_size: int, default = -1
        The maximum number of items of a sequence to be retained.
        If `min_sequence_size = -1`, all sequences will be included.

    bin_threshold: float, default = None
        The rating threshold to binarize rating values (turn explicit feedback to implicit feedback).
        For example, if `bin_threshold = 3.0`, all rating values >= 3.0 will be set to 1.0,
        and the rest (< 3.0) will be discarded.

    encoding: str, default = `utf-8`
        Encoding used to decode the file.

    errors: int, default = None
        Optional string that specifies how encoding errors are to be handled.
        Pass 'strict' to raise a ValueError exception if there is an encoding error
        (None has the same effect), or pass 'ignore' to ignore errors.
    """

    def __init__(
        self,
        user_set=None,
        item_set=None,
        min_user_freq=1,
        min_item_freq=1,
        num_top_freq_user=0,
        num_top_freq_item=0,
        min_basket_size=1,
        max_basket_size=-1,
        min_basket_sequence=1,
        min_sequence_size=1,
        max_sequence_size=-1,
        bin_threshold=None,
        encoding="utf-8",
        errors=None,
    ):
        self.user_set = (
            user_set
            if (user_set is None or isinstance(user_set, set))
            else set(user_set)
        )
        self.item_set = (
            item_set
            if (item_set is None or isinstance(item_set, set))
            else set(item_set)
        )
        self.min_uf = min_user_freq
        self.min_if = min_item_freq
        self.num_top_freq_user = num_top_freq_user
        self.num_top_freq_item = num_top_freq_item
        self.min_basket_size = min_basket_size
        self.max_basket_size = max_basket_size
        self.min_basket_sequence = min_basket_sequence
        self.min_sequence_size = min_sequence_size
        self.max_sequence_size = max_sequence_size
        self.bin_threshold = bin_threshold
        self.encoding = encoding
        self.errors = errors

    def _filter(self, tuples, fmt="UIR"):
        i_pos = fmt.find("I")
        u_pos = fmt.find("U")
        r_pos = fmt.find("R")

        if self.bin_threshold is not None and r_pos >= 0:

            def binarize(t):
                t = list(t)
                t[r_pos] = 1.0
                return tuple(t)

            tuples = [binarize(t) for t in tuples if t[r_pos] >= self.bin_threshold]

        if self.num_top_freq_user > 0:
            user_freq = Counter(t[u_pos] for t in tuples)
            top_freq_users = set(
                k for (k, _) in user_freq.most_common(self.num_top_freq_user)
            )
            tuples = [t for t in tuples if t[u_pos] in top_freq_users]

        if self.num_top_freq_item > 0:
            item_freq = Counter(t[i_pos] for t in tuples)
            top_freq_items = set(
                k for (k, _) in item_freq.most_common(self.num_top_freq_item)
            )
            tuples = [t for t in tuples if t[i_pos] in top_freq_items]

        if self.user_set is not None:
            tuples = [t for t in tuples if t[u_pos] in self.user_set]

        if self.item_set is not None:
            tuples = [t for t in tuples if t[i_pos] in self.item_set]

        if self.min_uf > 1:
            user_freq = Counter(t[u_pos] for t in tuples)
            tuples = [t for t in tuples if user_freq[t[u_pos]] >= self.min_uf]

        if self.min_if > 1:
            item_freq = Counter(t[i_pos] for t in tuples)
            tuples = [t for t in tuples if item_freq[t[i_pos]] >= self.min_if]

        return tuples

    def _filter_basket(self, tuples, fmt="UBI"):
        u_pos = fmt.find("U")
        b_pos = fmt.find("B")

        if self.min_basket_size > 1:
            sizes = Counter(t[b_pos] for t in tuples)
            tuples = [t for t in tuples if sizes[t[b_pos]] >= self.min_basket_size]

        if self.max_basket_size > 1:
            sizes = Counter(t[b_pos] for t in tuples)
            tuples = [t for t in tuples if sizes[t[b_pos]] <= self.max_basket_size]

        if self.min_basket_sequence > 1:
            basket_sequence = Counter(
                u for (u, _) in set((t[u_pos], t[b_pos]) for t in tuples)
            )
            tuples = [
                t
                for t in tuples
                if basket_sequence[t[u_pos]] >= self.min_basket_sequence
            ]

        return tuples

    def _filter_sequence(self, tuples, fmt="SIT"):
        s_pos = fmt.find("S")

        if self.min_sequence_size > 1:
            sizes = Counter(t[s_pos] for t in tuples)
            tuples = [t for t in tuples if sizes[t[s_pos]] >= self.min_sequence_size]

        if self.max_sequence_size > 1:
            sizes = Counter(t[s_pos] for t in tuples)
            tuples = [t for t in tuples if sizes[t[s_pos]] <= self.max_sequence_size]

        return tuples

    def read(
        self,
        fpath,
        fmt="UIR",
        sep="\t",
        skip_lines=0,
        id_inline=False,
        parser=None,
        **kwargs
    ):
        """Read data and parse line by line based on provided `fmt` or `parser`.

        Parameters
        ----------
        fpath: str
            Path to the data file.

        fmt: str, default: 'UIR'
            Line format to be parsed ('UI', 'UIR', 'UIRT', 'UITup', 'UIReview', 'UBI', 'UBIT', or 'UBITJson')

        sep: str, default: '\t'
            The delimiter string.

        skip_lines: int, default: 0
            Number of first lines to skip

        id_inline: bool, default: False
            If `True`, user ids corresponding to the line numbers of the file,
            where all the ids in each line are item ids.

        parser: function, default: None
            Function takes a list of `str` tokenized by `sep` and
            returns a list of tuples which will be joined to the final results.
            If `None`, parser will be determined based on `fmt`.

        Returns
        -------
        tuples: list
            Data in the form of list of tuples. What inside each tuple
            depends on `parser` or `fmt`.

        """
        parser = PARSERS.get(fmt, None) if parser is None else parser
        if parser is None:
            raise ValueError(
                "Invalid line format: {}\n"
                "Supported formats: {}".format(fmt, PARSERS.keys())
            )

        with open(fpath, encoding=self.encoding, errors=self.errors) as f:
            tuples = [
                tup
                for idx, line in enumerate(itertools.islice(f, skip_lines, None))
                for tup in parser(
                    line.strip().split(sep), line_idx=idx, id_inline=id_inline, **kwargs
                )
            ]
            tuples = self._filter(tuples=tuples, fmt=fmt)
            if fmt in {"UBI", "UBIT", "UBITJson"}:
                tuples = self._filter_basket(tuples=tuples, fmt=fmt)
            elif fmt in {"SIT", "SITJson", "USIT", "USITJson"}:
                tuples = self._filter_sequence(tuples=tuples, fmt=fmt)
            return tuples


def read_text(fpath, sep=None, encoding="utf-8", errors=None):
    """Read text file and return two lists of text documents and corresponding ids.
    If `sep` is None, only return one list containing elements are lines of text
    in the original file.

    Parameters
    ----------
    fpath: str
        Path to the data file

    sep: str, default = None
        The delimiter string used to split `id` and `text`. Each line is assumed
        containing an `id` followed by corresponding `text` document.
        If `None`, each line will be a `str` in returned list.

    encoding: str, default = `utf-8`
        Encoding used to decode the file.

    errors: int, default = None
        Optional string that specifies how encoding errors are to be handled.
        Pass 'strict' to raise a ValueError exception if there is an encoding error
        (None has the same effect), or pass 'ignore' to ignore errors.

    Returns
    -------
    texts, ids (optional): list, list
        Return list of text strings with corresponding indices (if `sep` is not None).
    """
    with open(fpath, encoding=encoding, errors=errors) as f:
        if sep is None:
            return [line.strip() for line in f]
        else:
            texts, ids = [], []
            for line in f:
                tokens = line.strip().split(sep)
                ids.append(tokens[0])
                texts.append(sep.join(tokens[1:]))
            return texts, ids

import os
import shutil
import zipfile
import tarfile
from urllib import request

from tqdm.auto import tqdm

def validate_format(input_format, valid_formats):
    """Check the input format is in list of valid formats
    :raise ValueError if not supported
    """
    if not input_format in valid_formats:
        raise ValueError('{} data format is not in valid formats ({})'.format(input_format, valid_formats))

    return input_format

def get_cache_path(relative_path, cache_dir=None):
    """Return the absolute path to the cached data file
    """
    if cache_dir is None and os.access(os.path.expanduser("~"), os.W_OK):
        cache_dir = os.path.join(os.path.expanduser("~"), ".dataset")
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)

    if not os.access(cache_dir, os.W_OK):
        cache_dir = os.path.join("/tmp", ".dataset")
    cache_path = os.path.join(cache_dir, relative_path)

    if not os.path.exists(os.path.dirname(cache_path)):
        os.makedirs(os.path.dirname(cache_path))

    return cache_path, cache_dir

def _urlretrieve(url, fpath):
    """Retrieve data from given url

    Parameters
    ----------
    url: str
        The url to the data.

    fpath: str
        The path to file where data is stored.

    """
    opener = request.build_opener()
    opener.addheaders = [("User-agent", "Mozilla/5.0")]

    with tqdm(unit="B", unit_scale=True) as progress:

        def report(chunk, chunksize, total):
            progress.total = total
            progress.update(chunksize)

        request.install_opener(opener)
        request.urlretrieve(url, fpath, reporthook=report)

def _extract_archive(file_path, extract_path="."):
    """Extracts an archive.
    """
    for archive_type in ["zip", "tar"]:
        if archive_type == "zip":
            open_fn = zipfile.ZipFile
            is_match_fn = zipfile.is_zipfile
        elif archive_type == "tar":
            open_fn = tarfile.open
            is_match_fn = tarfile.is_tarfile

        if is_match_fn(file_path):
            with open_fn(file_path) as archive:
                try:
                    archive.extractall(extract_path)
                except (tarfile.TarError, RuntimeError, KeyboardInterrupt):
                    if os.path.exists(extract_path):
                        if os.path.isfile(extract_path):
                            os.remove(extract_path)
                        else:
                            shutil.rmtree(extract_path)
                    raise

def cache(url, unzip=False, relative_path=None, cache_dir=None):
  """Download the data and cache to file

  Parameters
  ----------
  url: str
      The url to the data.

  unzip: bool, optional, default: False
      Whether the data is a zip file and going to be unzipped after the download.

  relative_path: str
      Relative path to the data file after finishing the download.
      If unzip=True, relative_path is the path to unzipped file.

  cache_dir: str, optional, default: None
      The path to cache folder. If `None`, either ~/.dataset or /tmp/.dataset will be used.

  """
  if relative_path is None:
      relative_path = url.split("/")[-1]
  cache_path, cache_dir = get_cache_path(relative_path, cache_dir)
  if os.path.exists(cache_path):
      return cache_path

  print("Data from", url)
  print("will be cached into", cache_path)

  if unzip:
      tmp_path = os.path.join(cache_dir, "file.tmp")
      _urlretrieve(url, tmp_path)
      print("Unzipping ...")
      _extract_archive(tmp_path, cache_dir)
      os.remove(tmp_path)
  else:
      _urlretrieve(url, cache_path)

  print("File cached!")
  return cache_path

# ratio split
from math import ceil
from collections import OrderedDict
import time

import numpy as np
from scipy.sparse import csr_matrix
from tqdm.auto import tqdm

class Modality:
    """Generic class of Modality to extend from
    """

    def __init__(self, **kwargs):
        pass

def fallback_feature(func):
    """Decorator to fallback to `batch_feature` in FeatureModality
    """

    def wrapper(self, *args, **kwargs):
        if self.features is not None:
            ids = args[0] if len(args) > 0 else kwargs['batch_ids']
            return FeatureModality.batch_feature(self, batch_ids=ids)
        else:
            return func(self, *args, **kwargs)

    return wrapper

class FeatureModality(Modality):
    """Modality that contains features in general

    Parameters
    ----------
    features: numpy.ndarray or scipy.sparse.csr_matrix, default = None
        Numpy 2d-array that the row indices are aligned with user/item in `ids`.

    ids: List, default = None
        List of user/item ids that the indices are aligned with `corpus`.
        If None, the indices of provided `features` will be used as `ids`.

    normalized: bool, default = False
        Whether the features will be normalized using min-max normalization.
    """

    def __init__(self, features=None, ids=None, normalized=False, **kwargs):
        super().__init__(**kwargs)
        self.features = features
        self.ids = ids
        self.normalized = normalized

    @property
    def features(self):
        """Return the whole feature matrix
        """
        return self.__features

    @features.setter
    def features(self, input_features):
        if input_features is not None:
            assert len(input_features.shape) == 2
        self.__features = input_features

    @property
    def feature_dim(self):
        """Return the dimensionality of the feature vectors
        """
        return self.features.shape[1]

    def _swap_feature(self, id_map):
        new_feats = np.copy(self.features)
        new_ids = self.ids.copy()
        for old_idx, raw_id in enumerate(self.ids):
            new_idx = id_map.get(raw_id, None)
            if new_idx is None:
                continue
            assert new_idx < new_feats.shape[0]
            new_feats[new_idx] = self.features[old_idx]
            new_ids[new_idx] = raw_id
        self.features = new_feats
        self.ids = new_ids

    def build(self, id_map=None, **kwargs):
        """Build the feature matrix.
        Features will be swapped if the id_map is provided
        """
        if self.features is None:
            return

        if (self.ids is not None) and (id_map is not None):
            self._swap_feature(id_map)

        if self.normalized:
            self.features = self.features - np.min(self.features)
            self.features = self.features / (np.max(self.features) + 1e-10)

        return self

    def batch_feature(self, batch_ids):
        """Return a matrix (batch of feature vectors) corresponding to provided batch_ids
        """
        assert self.features is not None
        return self.features[batch_ids]

# command utils
# Copyright 2018 The Cornac Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

import numbers

import numpy as np
import scipy.sparse as sp

# from .fast_sparse_funcs import (
#     inplace_csr_row_normalize_l1,
#     inplace_csr_row_normalize_l2
# )

FLOAT_DTYPES = (np.float64, np.float32, np.float16)


def sigmoid(x):
    """Sigmoid function"""
    return 1. / (1. + np.exp(-x))


def scale(values, target_min, target_max, source_min=None, source_max=None):
    """Scale the value of a numpy array "values"
    from source_min, source_max into a range [target_min, target_max]

    Parameters
    ----------
    values : Numpy array, required
        Values to be scaled.

    target_min : scalar, required
        Target minimum value.

    target_max : scalar, required
        Target maximum value.

    source_min : scalar, required, default: None
        Source minimum value if desired. If None, it will be the minimum of values.

    source_max : scalar, required, default: None
        Source minimum value if desired. If None, it will be the maximum of values.

    Returns
    -------
    res: Numpy array
        Output values mapped into range [target_min, target_max]
    """
    if source_min is None:
        source_min = np.min(values)
    if source_max is None:
        source_max = np.max(values)
    if source_min == source_max:  # improve this scenario
        source_min = 0.0

    values = (values - source_min) / (source_max - source_min)
    values = values * (target_max - target_min) + target_min
    return values


def clip(values, lower_bound, upper_bound):
    """Perform clipping to enforce values to lie
    in a specific range [lower_bound, upper_bound]

    Parameters
    ----------
    values : Numpy array, required
        Values to be clipped.

    lower_bound : scalar, required
        Lower bound of the output.

    upper_bound : scalar, required
        Upper bound of the output.

    Returns
    -------
    res: Numpy array
        Clipped values in range [lower_bound, upper_bound]
    """
    values = np.where(values > upper_bound, upper_bound, values)
    values = np.where(values < lower_bound, lower_bound, values)

    return values


def intersects(x, y, assume_unique=False):
    """Return the intersection of given two arrays
    """
    mask = np.in1d(x, y, assume_unique=assume_unique)
    x_intersects_y = x[mask]

    return x_intersects_y


def excepts(x, y, assume_unique=False):
    """Removing elements in array y from array x
    """
    mask = np.in1d(x, y, assume_unique=assume_unique, invert=True)
    x_excepts_y = x[mask]

    return x_excepts_y


def safe_indexing(X, indices):
    """Return items or rows from X using indices.
    Allows simple indexing of lists or arrays.
    Borrowed from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/__init__.py

    Parameters
    ----------
    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series.
        Data from which to sample rows or items.
    indices : array-like of int
        Indices according to which X will be subsampled.

    Returns
    -------
    subset
        Subset of X on first axis
    """
    if hasattr(X, "shape"):
        if hasattr(X, 'take') and (hasattr(indices, 'dtype') and
                                   indices.dtype.kind == 'i'):
            # This is often substantially faster than X[indices]
            return X.take(indices, axis=0)
        else:
            return X[indices]
    else:
        return [X[idx] for idx in indices]


def validate_format(input_format, valid_formats):
    """Check the input format is in list of valid formats
    :raise ValueError if not supported
    """
    if not input_format in valid_formats:
        raise ValueError('{} data format is not in valid formats ({})'.format(input_format, valid_formats))

    return input_format


def estimate_batches(input_size, batch_size):
    """
    Estimate number of batches give `input_size` and `batch_size`
    """
    return int(np.ceil(input_size / batch_size))


def get_rng(seed):
    '''Return a RandomState of Numpy.
    If seed is None, use RandomState singleton from numpy.
    If seed is an integer, create a RandomState from that seed.
    If seed is already a RandomState, just return it.
    '''
    if seed is None:
        return np.random.mtrand._rand
    if isinstance(seed, (numbers.Integral, np.integer)):
        return np.random.RandomState(seed)
    if isinstance(seed, np.random.RandomState):
        return seed
    raise ValueError('{} can not be used to create a numpy.random.RandomState'.format(seed))


def normalize(X, norm='l2', axis=1, copy=True):
    """Scale input vectors individually to unit norm (vector length).

    Parameters
    ----------
    X : {array-like, sparse matrix}, shape [n_samples, n_features]
        The data to normalize, element by element.
        scipy.sparse matrices should be in CSR format to avoid an
        un-necessary copy.

    norm : 'l1', 'l2', or 'max', optional ('l2' by default)
        The norm to use to normalize each non zero sample (or each non-zero
        feature if axis is 0).

    axis : 0 or 1, optional (1 by default)
        axis used to normalize the data along. If 1, independently normalize
        each sample, otherwise (if 0) normalize each feature.

    copy : boolean, optional, default True
        set to False to perform inplace row normalization and avoid a
        copy (if the input is already a numpy array or a scipy.sparse
        CSR matrix and if axis is 1).

    Reference
    ---------
    https://github.com/scikit-learn/scikit-learn/blob/1495f69242646d239d89a5713982946b8ffcf9d9/sklearn/preprocessing/data.py#L1553

    """
    if norm not in ('l1', 'l2', 'max'):
        raise ValueError("'%s' is not a supported norm" % norm)

    if len(X.shape) != 2:
        raise ValueError("input X must be 2D but shape={}".format(X.shape))

    X_out = X.copy() if copy else X
    X_out = X_out if X_out.dtype in FLOAT_DTYPES else X_out.astype(np.float64)

    if axis == 0:
        X_out = X_out.T

    if sp.issparse(X_out):
        X_out = X_out.tocsr()

        if norm == 'l1':
            # inplace_csr_row_normalize_l1(X_out)
            print('l1')
        elif norm == 'l2':
            print('l2')
            # inplace_csr_row_normalize_l2(X_out)
        elif norm == 'max':
            norms = X_out.max(axis=1).A
            norms_elementwise = norms.repeat(np.diff(X_out.indptr))
            mask = norms_elementwise != 0
            X_out.data[mask] /= norms_elementwise[mask]
    else:
        if norm == 'l1':
            norms = np.abs(X_out).sum(axis=1)
        elif norm == 'l2':
            norms = np.sqrt((X_out ** 2).sum(axis=1))
        elif norm == 'max':
            norms = np.max(X_out, axis=1)
        norms[norms == 0] = 1.
        X_out /= norms.reshape(-1, 1)

    if axis == 0:
        X_out = X_out.T

    return X_out

# SentimentModality
# Copyright 2018 The Cornac Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

from collections import OrderedDict


class SentimentModality(Modality):
    """Aspect module
    Parameters
    ----------
    data: List[tuple], required
        A triplet list of user, item, and sentiment information \
        which also a triplet list of aspect, opinion, and sentiment, \
        e.g., data=[('user1', 'item1', [('aspect1', 'opinion1', 'sentiment1')])].
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.raw_data = kwargs.get('data', OrderedDict())

    @property
    def sentiment(self):
        return self.__sentiment

    @sentiment.setter
    def sentiment(self, input_sentiment):
        self.__sentiment = input_sentiment

    @property
    def num_aspects(self):
        """Return the number of aspects"""
        return len(self.aspect_id_map)

    @property
    def num_opinions(self):
        """Return the number of aspects"""
        return len(self.opinion_id_map)

    @property
    def user_sentiment(self):
        return self.__user_sentiment

    @user_sentiment.setter
    def user_sentiment(self, input_user_sentiment):
        self.__user_sentiment = input_user_sentiment

    @property
    def item_sentiment(self):
        return self.__item_sentiment

    @item_sentiment.setter
    def item_sentiment(self, input_item_sentiment):
        self.__item_sentiment = input_item_sentiment

    @property
    def aspect_id_map(self):
        return self.__aspect_id_map

    @aspect_id_map.setter
    def aspect_id_map(self, input_aspect_id_map):
        self.__aspect_id_map = input_aspect_id_map

    @property
    def opinion_id_map(self):
        return self.__opinion_id_map

    @opinion_id_map.setter
    def opinion_id_map(self, input_opinion_id_map):
        self.__opinion_id_map = input_opinion_id_map

    def _build_sentiment(self, uid_map, iid_map, dok_matrix):
        self.user_sentiment = OrderedDict()
        self.item_sentiment = OrderedDict()
        aid_map = OrderedDict()
        oid_map = OrderedDict()
        sentiment = OrderedDict()
        for idx, (raw_uid, raw_iid, sentiment_tuples) in enumerate(self.raw_data):
            user_idx = uid_map.get(raw_uid, None)
            item_idx = iid_map.get(raw_iid, None)
            if user_idx is None or item_idx is None or dok_matrix[user_idx, item_idx] == 0:
                continue
            user_dict = self.user_sentiment.setdefault(user_idx, OrderedDict())
            user_dict[item_idx] = idx
            item_dict = self.item_sentiment.setdefault(item_idx, OrderedDict())
            item_dict[user_idx] = idx

            mapped_tup = []
            for tup in sentiment_tuples:
                aspect, opinion, polarity = tup[0], tup[1], float(tup[2])
                aspect_idx = aid_map.setdefault(aspect, len(aid_map))
                opinion_idx = oid_map.setdefault(opinion, len(oid_map))
                mapped_tup.append((aspect_idx, opinion_idx, polarity))
            sentiment.setdefault(idx, mapped_tup)

        self.sentiment = sentiment
        self.aspect_id_map = aid_map
        self.opinion_id_map = oid_map

    def build(self, uid_map=None, iid_map=None, dok_matrix=None, **kwargs):
        """Build the model based on provided list of ordered ids
        """
        if uid_map is not None and iid_map is not None and dok_matrix is not None:
            self._build_sentiment(uid_map, iid_map, dok_matrix)
        return self

# Copyright 2018 The Cornac Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

from typing import List, Dict, Callable, Union
from collections import defaultdict, Counter, OrderedDict
import string
import pickle
import re

import numpy as np
import scipy.sparse as sp
from pyvi import ViTokenizer

# from . import FeatureModality
# from .modality import fallback_feature
# from ..utils import normalize

__all__ = ['Tokenizer',
           'BaseTokenizer',
           'Vocabulary',
           'CountVectorizer',
           'TextModality']

PAD, UNK, BOS, EOS = '<PAD>', '<UNK>', '<BOS>', '<EOS>'
SPECIAL_TOKENS = [PAD, UNK, BOS, EOS]

ENGLISH_STOPWORDS = frozenset([
    'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',
    'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',
    'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',
    'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',
    'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',
    'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',
    'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven',
    'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything',
    'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for',
    'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give',
    'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein',
    'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if',
    'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter',
    'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine',
    'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely',
    'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not',
    'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other',
    'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps',
    'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious',
    'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow',
    'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten',
    'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby',
    'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though',
    'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards',
    'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',
    'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas',
    'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever',
    'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your',
    'yours', 'yourself', 'yourselves'])


# VIETNAMESE_STOPWORDS = frozenset([
#     'là', 'của', 'và', 'có', 'trong', 'một', 'những', 'được', 'không', 'cho', 'với', 'ngày', 'này', 'các',
#     'từ', 'về', 'người', 'thì', 'đã', 'bởi', 'năm', 'sau', 'khi', 'tại', 'ra', 'lại', 'nhiều', 'như', 'tháng',
#     'trên', 'nhưng', 'vẫn', 'hơn', 'nữa', 'rằng', 'đây', 'nơi', 'nó', 'mà', 'cũng', 'lên', 'vào', 'đó', 'để',
#     'đến', 'vừa', 'mỗi', 'hay', 'nếu', 'còn', 'thì', 'chỉ', 'bạn', 'mình', 'chúng', 'ta', 'tôi', 'em', 'anh',
#     'chào', 'ông', 'bà', 'cô', 'thầy', 'gì', 'ai', 'điều', 'gì', 'đi', 'ngay', 'cùng', 'nào', 'sao', 'tất',
#     'cả', 'vậy', 'ấy', 'được', 'rồi', 'đâu', 'phải', 'thế', 'nào', 'ở', 'về', 'dưới', 'lúc', 'giờ', 'của',
#     'ngay', 'chưa', 'được', 'phải', 'làm', 'được', 'mà'
# ])

with open('./vietnamese-stopwords-dash.txt', 'r', encoding='utf-8') as file:
    VIETNAMESE_STOPWORDS = [line.strip() for line in file if line.strip()]
VIETNAMESE_STOPWORDS = frozenset(VIETNAMESE_STOPWORDS)

def _validate_stopwords(stop_words):
    if stop_words == 'english':
        return ENGLISH_STOPWORDS
    if stop_words == 'vietnamese':
        return VIETNAMESE_STOPWORDS
    elif isinstance(stop_words, str):
        raise ValueError("Invalid built-in stop-words list: %s" % stop_words)
    elif stop_words is None:
        return None
    else:
        return frozenset(stop_words)


class Tokenizer():
    """
    Generic class for other subclasses to extend from. This typically
    either splits text into word tokens or character tokens.
    """

    def tokenize(self, t: str) -> List[str]:
        """
        Splitting text into tokens.

        Parameters
        ----------
        t: str, required
            Input text to be tokenized.

        Returns
        -------
        tokens : ``List[str]``
        """
        raise NotImplementedError

    def batch_tokenize(self, texts: List[str]) -> List[List[str]]:
        """
        Splitting a corpus with multiple text documents.

        Parameters
        ----------
        texts: List[str], required
            Input list of texts to be tokenized.

        Returns
        -------
        tokens : ``List[List[str]]``
        """
        raise NotImplementedError


def rm_tags(t: str) -> str:
    """
    Remove html tags.
    e,g, rm_tags("<i>Hello</i> <b>World</b>!") -> "Hello World".
    """
    return re.sub('<([^>]+)>', '', t)


def rm_numeric(t: str) -> str:
    """
    Remove digits from `t`.
    """
    return re.sub('[0-9]+', ' ', t)


def rm_punctuation(t: str) -> str:
    """
    Remove "!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~" from t.
    """
    return t.translate(str.maketrans('', '', string.punctuation))


def rm_dup_spaces(t: str) -> str:
    """
    Remove duplicate spaces in `t`.
    """
    return re.sub(' {2,}', ' ', t)

def rm_special_chars(t: str) -> str:
    """
    Remove special characters like “ ”, –, …, • from `t`.
    """
    return re.sub(r'[“”–…•★‘’]', '', t)

def rm_exception(t: str):
        document = t.lower()
        document = re.sub(r'https?://\S+|www\.\S+', ' ', document)
        document = re.sub(r'<.*?>', ' ', document)
        document = re.sub(r'\[.*?\]', ' ', document)
        document = re.sub(r'\n', ' ', document)
        document = re.sub(r'\w*\d\w*', ' ', document)

        document = re.sub(r'[^\s\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)
        document = re.sub(r'\s+', ' ', document).strip()
        return document


DEFAULT_PRE_RULES = [lambda t: t.lower(), rm_tags, rm_numeric, rm_punctuation, rm_dup_spaces,rm_special_chars, rm_exception]


class BaseTokenizer(Tokenizer):
    """
    A base tokenizer use a provided delimiter `sep` to split text.

    Parameters
    ----------
    sep: str, optional, default: ' '
        Separator string used to split text into tokens.

    pre_rules: List[Callable[[str], str]], optional
        List of callable lambda functions to apply on text before tokenization.

    stop_words: Union[List, str], optional
        List of stop-words to be ignored during tokenization,
        or key of built-in stop-word lists (e.g., english).

    """

    def __init__(self, sep: str = ' ',
                 pre_rules: List[Callable[[str], str]] = None,
                 stop_words: Union[List, str] = None):
        self.sep = sep
        self.pre_rules = DEFAULT_PRE_RULES if pre_rules is None else pre_rules
        self.stop_words = _validate_stopwords(stop_words)

    def tokenize(self, t: str) -> List[str]:
        """
        Splitting text into tokens.

        Parameters
        ----------
        t: str, required
            Input text to be tokenized.

        Returns
        -------
        tokens : ``List[str]``
        """
#         print('tokenize def')
        for rule in self.pre_rules:
            t = rule(t)
#         print('stop_words: ', self.sep)
#         if self.stop_words == 'vietnamese':
#             print('step_word = vietnamese')
#             tokens = ViTokenizer.tokenize(t).split()
#         else:
#             tokens = t.split(self.sep)
#         tokens = t.split(self.sep)
        tokens = ViTokenizer.tokenize(t).split()
        if self.stop_words is not None:
            tokens = [tok for tok in tokens if tok not in self.stop_words]
        return tokens

    # TODO: this function can be parallelized
    def batch_tokenize(self, texts: List[str]) -> List[List[str]]:
        """
        Splitting a corpus with multiple text documents.

        Parameters
        ----------
        texts: List[str], required
            Input list of texts to be tokenized.

        Returns
        -------
        tokens : ``List[List[str]]``
        """
        return [self.tokenize(t) for t in texts]


class Vocabulary():
    """
    Vocabulary basically contains mapping between numbers and tokens and vice versa.

    Parameters
    ----------
    idx2tok: List[str], required
        List of tokens where list indices are corresponding to
        their mapped integer indices.

    use_special_tokens: bool, optional, default: False
        If `True`, vocabulary will include `SPECIAL_TOKENS`.

    """

    def __init__(self, idx2tok: List[str], use_special_tokens: bool = False):
        self.use_special_tokens = use_special_tokens
        self.idx2tok = self._add_special_tokens(idx2tok) if use_special_tokens else idx2tok
        self.build_tok2idx()

    def build_tok2idx(self):
        """
        Build a mapping between tokens to their integer indices
        """
        self.tok2idx = defaultdict(int, {tok: idx for idx, tok in enumerate(self.idx2tok)})
        print('list token to idx')
        for i, (tok, idx) in enumerate(self.tok2idx.items()):
            if i >= 10:
                break
            print(f"{tok}: {idx}")

    @staticmethod
    def _add_special_tokens(idx2tok: List[str]) -> List[str]:
        for tok in reversed(SPECIAL_TOKENS):  # <PAD>:0, '<UNK>':1, '<BOS>':2, '<EOS>':3
            if tok in idx2tok:
                idx2tok.remove(tok)
            idx2tok.insert(0, tok)
        return idx2tok

    @property
    def size(self):
        return len(self.idx2tok)

    def to_idx(self, tokens: List[str]) -> List[int]:
        """Convert a list of `tokens` to their integer indices.

        Parameters
        ----------
        tokens: List[str], required
            List of string tokens.

        Returns
        -------
        indices: List[int]
            List of integer indices corresponding to input `tokens`.

        """
        return [self.tok2idx.get(tok, 1) for tok in tokens]  # 1 is <UNK> idx

    def to_text(self, indices: List[int], sep=' ') -> List[str]:
        """Convert a list of integer `indices` to their tokens.

        Parameters
        ----------
        indices: List[int], required
            List of token integer indices.

        sep: str, optional, default: ' '
            Separator string used to connect tokens.

        Returns
        -------
        text: str
            Aggregated text of tokens seperated by `sep`.

        """
        return sep.join([self.idx2tok[i] for i in indices]) if sep is not None else [self.idx2tok[i] for i in indices]

    def save(self, path):
        """Save idx2tok into a pickle file.

        Parameters
        ----------
        path: str, required
            Path to store the dictionary on disk.

        """
        pickle.dump(self.idx2tok, open(path, 'wb'))

    @classmethod
    def load(cls, path):
        """
        Load a vocabulary from `path` to a pickle file.
        """
        return cls(pickle.load(open(path, 'rb')))

    @classmethod
    def from_tokens(cls, tokens: List[str],
                    max_vocab: int = None,
                    min_freq: int = 1,
                    use_special_tokens: bool = False) -> 'Vocabulary':
        """Build a vocabulary from list of tokens.

        Parameters
        ----------
        tokens: List[str], required
            List of string tokens.

        max_vocab: int, optional
            Limit for size of the vocabulary. If specified, tokens will
            be ranked based on counts and gathered top-down until reach `max_vocab`.

        min_freq: int, optional, default: 1
            Cut-off threshold for tokens based on their counts.

        use_special_tokens: bool, optional, default: False
            If `True`, vocabulary will include `SPECIAL_TOKENS`.

        """
        freq = Counter(tokens)
        idx2tok = [tok for tok, cnt in freq.most_common(max_vocab) if cnt >= min_freq]
        return cls(idx2tok, use_special_tokens)

    @classmethod
    def from_sequences(cls, sequences: List[List[str]],
                       max_vocab: int = None,
                       min_freq: int = 1,
                       use_special_tokens: bool = False) -> 'Vocabulary':
        """Build a vocabulary from sequences (list of list of tokens).

        Parameters
        ----------
        sequences: List[List[str]], required
            Corpus of multiple lists of string tokens.

        max_vocab: int, optional
            Limit for size of the vocabulary. If specified, tokens will
            be ranked based on counts and gathered top-down until reach `max_vocab`.

        min_freq: int, optional, default: 1
            Cut-off threshold for tokens based on their counts.

        use_special_tokens: bool, optional, default: False
            If `True`, vocabulary will include `SPECIAL_TOKENS`.

        """
        return Vocabulary.from_tokens([tok for seq in sequences for tok in seq],
                                      max_vocab, min_freq, use_special_tokens)


class CountVectorizer():
    """Convert a collection of text documents to a matrix of token counts
    This implementation produces a sparse representation of the counts using
    scipy.sparse.csr_matrix.

    Parameters
    ----------
    tokenizer: Tokenizer, optional, default=None
        Tokenizer for text splitting. If None, the BaseTokenizer will be used.

    vocab: Vocabulary, optional, default = None
        Vocabulary of tokens. It contains mapping between tokens to their
        integer ids and vice versa.

    max_doc_freq: float in range [0.0, 1.0] or int, default=1.0
        When building the vocabulary ignore terms that have a document
        frequency strictly higher than the given threshold (corpus-specific
        stop words).
        If float, the value represents a proportion of documents, int for absolute counts.
        If `vocab` is not None, this will be ignored.

    min_doc_freq: float in range [0.0, 1.0] or int, default=1
        When building the vocabulary ignore terms that have a document
        frequency strictly lower than the given threshold. This value is also
        called cut-off in the literature.
        If float, the value represents a proportion of documents, int absolute counts.
        If `vocab` is not None, this will be ignored.

    max_features : int or None, optional, default=None
        If not None, build a vocabulary that only consider the top
        `max_features` ordered by term frequency across the corpus.
        If `vocab` is not None, this will be ignored.

    binary : boolean, default=False
        If True, all non zero counts are set to 1.

    Reference
    ---------
    https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L790

    """

    def __init__(self,
                 tokenizer: Tokenizer = None,
                 vocab: Vocabulary = None,
                 max_doc_freq: Union[float, int] = 1.0,
                 min_doc_freq: int = 1,
                 max_features: int = None,
                 binary: bool = False):
        self.tokenizer = BaseTokenizer() if tokenizer is None else tokenizer
        self.vocab = vocab
        self.max_doc_freq = max_doc_freq
        self.min_doc_freq = min_doc_freq
        if max_doc_freq < 0 or min_doc_freq < 0:
            raise ValueError('negative value for max_doc_freq or min_doc_freq')
        self.max_features = max_features
        if max_features is not None:
            if max_features <= 0:
                raise ValueError('max_features=%r, '
                                 'neither a positive integer nor None' % max_features)
        self.binary = binary

    def _limit_features(self, X: sp.csr_matrix, max_doc_count: int, min_doc_count: int):
        """Remove too common features.
        Prune features that are non zero in more samples than max_doc_count
        and modifying the vocabulary.
        """
        if max_doc_count >= X.shape[0] and min_doc_count <= 1 and self.max_features is None:
            return X

        # Calculate a mask based on document frequencies
        doc_freq = np.bincount(X.indices, minlength=X.shape[1])
        term_indices = np.arange(X.shape[1])  # terms are already sorted based on frequency from Vocabulary
        mask = np.ones(len(doc_freq), dtype=bool)
        if max_doc_count < X.shape[0]:
            mask &= doc_freq <= max_doc_count
        if min_doc_count > 1:
            mask &= doc_freq >= min_doc_count

        if self.max_features is not None and mask.sum() > self.max_features:
            mask_indices = term_indices[mask][:self.max_features]
            new_mask = np.zeros(len(doc_freq), dtype=bool)
            new_mask[mask_indices] = True
            mask = new_mask

        for index in np.sort(np.where(np.logical_not(mask))[0])[::-1]:
            del self.vocab.idx2tok[index]
        self.vocab.build_tok2idx()  # rebuild the mapping

        kept_indices = np.where(mask)[0]
        if len(kept_indices) == 0:
            raise ValueError("After pruning, no terms remain. Try a lower"
                             " min_freq or a higher max_doc_freq.")
        return X[:, kept_indices]

    def _count(self, sequences: List[List[str]]):
        """
        Create sparse feature matrix of document term counts
        Ignore SPECIAL_TOKENS if used from count matrix
        """
        data = []
        indices = []
        indptr = [0]
        for sequence in sequences:
            feature_counter = Counter()
            for token in sequence:
                if token not in self.vocab.tok2idx.keys():
                    continue
                idx = self.vocab.tok2idx[token]
                if self.vocab.use_special_tokens:
                    idx -= len(SPECIAL_TOKENS)
                feature_counter[idx] += 1

            indices.extend(feature_counter.keys())
            data.extend(feature_counter.values())
            indptr.append(len(indices))

        feature_dim = self.vocab.size
        if self.vocab.use_special_tokens:
            feature_dim -= len(SPECIAL_TOKENS)
        X = sp.csr_matrix((data, indices, indptr),
                          shape=(len(sequences), feature_dim),
                          dtype=np.int64)
        X.sort_indices()
        return X

    def fit(self, raw_documents: List[str]) -> 'CountVectorizer':
        """Build a vocabulary of all tokens in the raw documents.

        Parameters
        ----------
        raw_documents : iterable
            An iterable which yields either str, unicode or file objects.

        Returns
        -------
        count_vectorizer: :obj:`<cornac.data.text.CountVectorizer>`
            An object of type `CountVectorizer`.

        """
        self.fit_transform(raw_documents)
        return self

    def fit_transform(self, raw_documents: List[str]) -> (List[List[str]], sp.csr_matrix):
        """Build the vocabulary and return term-document matrix.

        Parameters
        ----------
        raw_documents : List[str]

        Returns
        -------
        (sequences, X) :
            sequences: List[List[str]
                Tokenized sequences of raw_documents
            X: array, [n_samples, n_features]
                Document-term matrix.
        """
        sequences = self.tokenizer.batch_tokenize(raw_documents)

        fixed_vocab = self.vocab is not None
        if self.vocab is None:
            self.vocab = Vocabulary.from_sequences(sequences)

        X = self._count(sequences)
        if self.binary:
            X.data.fill(1)

        if not fixed_vocab:
            n_docs = X.shape[0]
            max_doc_count = (self.max_doc_freq
                             if isinstance(self.max_doc_freq, int)
                             else int(self.max_doc_freq * n_docs))
            min_doc_count = (self.min_doc_freq
                             if isinstance(self.min_doc_freq, int)
                             else int(self.min_doc_freq * n_docs))
            X = self._limit_features(X, max_doc_count, min_doc_count)

        return sequences, X

    def transform(self, raw_documents: List[str]) -> (List[List[str]], sp.csr_matrix):
        """Transform documents to document-term matrix.

        Parameters
        ----------
        raw_documents : List[str]

        Returns
        -------
        (sequences, X) :
            sequences: List[List[str]
                Tokenized sequences of raw_documents.
            X: array, [n_samples, n_features]
                Document-term matrix.
        """
        sequences = self.tokenizer.batch_tokenize(raw_documents)
        X = self._count(sequences)
        if self.binary:
            X.data.fill(1)
        return sequences, X


class TfidfVectorizer(CountVectorizer):
    """Convert a collection of raw documents to a matrix of TF-IDF features.

    Parameters
    ----------
    tokenizer: Tokenizer, optional, default = None
        Tokenizer for text splitting. If None, the BaseTokenizer will be used.

    vocab: Vocabulary, optional, default = None
        Vocabulary of tokens. It contains mapping between tokens to their
        integer ids and vice versa.

    max_doc_freq: float in range [0.0, 1.0] or int, default=1.0
        When building the vocabulary ignore terms that have a document
        frequency strictly higher than the given threshold (corpus-specific
        stop words).
        If float, the value represents a proportion of documents, int for absolute counts.
        If `vocab` is not None, this will be ignored.

    min_doc_freq: float in range [0.0, 1.0] or int, default=1
        When building the vocabulary ignore terms that have a document
        frequency strictly lower than the given threshold. This value is also
        called cut-off in the literature.
        If float, the value represents a proportion of documents, int absolute counts.
        If `vocab` is not None, this will be ignored.

    max_features : int or None, optional, default=None
        If not None, build a vocabulary that only consider the top
        `max_features` ordered by term frequency across the corpus.
        If `vocab` is not None, this will be ignored.

    binary : boolean, default=False
        If True, all non zero counts are set to 1.

    norm : 'l1', 'l2' or None, optional, default='l2'
        Each output row will have unit norm, either:
        * 'l2': Sum of squares of vector elements is 1. The cosine
        similarity between two vectors is their dot product when l2 norm has
        been applied.
        * 'l1': Sum of absolute values of vector elements is 1.
        See :func:`utils.common.normalize`

    use_idf : boolean, default=True
        Enable inverse-document-frequency reweighting.

    smooth_idf : boolean, default=True
        Smooth idf weights by adding one to document frequencies, as if an
        extra document was seen containing every term in the collection
        exactly once. Prevents zero divisions.

    sublinear_tf : boolean (default=False)
        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).

    Reference
    ---------
    https://github.com/scikit-learn/scikit-learn/blob/d6d1d63fa6b098c72953a6827aae475f611936ed/sklearn/feature_extraction/text.py#L1451

    """

    def __init__(self,
                 tokenizer: Tokenizer = None,
                 vocab: Vocabulary = None,
                 max_doc_freq: Union[float, int] = 1.0,
                 min_doc_freq: int = 1,
                 max_features: int = None,
                 binary: bool = False,
                 norm='l2',
                 use_idf=True,
                 smooth_idf=True,
                 sublinear_tf=False):
        super().__init__(tokenizer=tokenizer,
                         vocab=vocab,
                         max_doc_freq=max_doc_freq,
                         min_doc_freq=min_doc_freq,
                         max_features=max_features,
                         binary=binary)

        self.norm = norm
        self.use_idf = use_idf
        self.smooth_idf = smooth_idf
        self.sublinear_tf = sublinear_tf

    def _build_idf(self, X):
        """
        Build inverse document frequency vector (global term weights).
        """
        n_samples, n_features = X.shape
        doc_freq = np.bincount(X.indices, minlength=X.shape[1])

        # perform idf smoothing if required
        doc_freq += int(self.smooth_idf)
        n_samples += int(self.smooth_idf)

        # log+1 instead of log makes sure terms with zero idf don't get
        # suppressed entirely.
        idf = np.log(n_samples / doc_freq) + 1

        self.idf = sp.diags(idf,
                            offsets=0,
                            shape=(n_features, n_features),
                            format='csr')

    def _transform(self, X):
        """
        Transform tf matrix into tf-idf matrix.
        """
        X = (X.tocsr().astype(np.float64)
             if sp.issparse(X)
             else sp.csr_matrix(X, dtype=np.float64))

        if self.sublinear_tf:
            np.log(X.data, X.data)
            X.data += 1

        if self.use_idf:
            X = X * self.idf

        if self.norm:
            X = normalize(X, norm=self.norm, copy=False)

        return X

    def fit(self, raw_documents: List[str]) -> 'TfidfVectorizer':
        """Build a vocabulary of all tokens in the raw documents.

        Parameters
        ----------
        raw_documents : iterable
            An iterable which yields either str, unicode or file objects.

        Returns
        -------
        tfidf_vectorizer: :obj:`<cornac.data.text.IfidfVectorizer>`
            An object of type `IfidfVectorizer`.

        """
        self.fit_transform(raw_documents)
        return self

    def fit_transform(self, raw_documents: List[str]) -> (List[List[str]], sp.csr_matrix):
        """Build the vocabulary and return term-document matrix.

        Parameters
        ----------
        raw_documents : List[str]

        Returns
        -------
        X : sparse matrix, [n_samples, n_features]
            Tf-idf-weighted document-term matrix.

        """
        _, X = super().fit_transform(raw_documents)

        if self.use_idf:
            self._build_idf(X)

        return self._transform(X)

    def transform(self, raw_documents: List[str]) -> (List[List[str]], sp.csr_matrix):
        """Transform documents to document-term matrix.

        Parameters
        ----------
        raw_documents : List[str]

        Returns
        -------
        X : sparse matrix, [n_samples, n_features]
            Tf-idf-weighted document-term matrix.

        """
        _, X = super().transform(raw_documents)
        return self._transform(X)


class TextModality(FeatureModality):
    """Text modality

    Parameters
    ----------
    corpus: List[str], default = None
        List of user/item texts that the indices are aligned with `ids`.

    ids: List, default = None
        List of user/item ids that the indices are aligned with `corpus`.
        If None, the indices of provided `corpus` will be used as `ids`.

    tokenizer: Tokenizer, optional, default = None
        Tokenizer for text splitting. If None, the BaseTokenizer will be used.

    vocab: Vocabulary, optional, default = None
        Vocabulary of tokens. It contains mapping between tokens to their
        integer ids and vice versa.

    max_vocab: int, optional, default = None
        The maximum size of the vocabulary.
        If vocab is provided, this will be ignored.

    max_doc_freq: float in range [0.0, 1.0] or int, default=1.0
        When building the vocabulary ignore terms that have a document
        frequency strictly higher than the given threshold (corpus-specific
        stop words).
        If float, the value represents a proportion of documents, int for absolute counts.
        If `vocab` is not None, this will be ignored.

    min_doc_freq: float in range [0.0, 1.0] or int, default=1
        When building the vocabulary ignore terms that have a document
        frequency strictly lower than the given threshold. This value is also
        called cut-off in the literature.
        If float, the value represents a proportion of documents, int absolute counts.
        If `vocab` is not None, this will be ignored.

    tfidf_params: dict or None, optional, default=None
        If `None`, a default arguments of :obj:`<cornac.data.text.IfidfVectorizer>` will be used.
        List of parameters:

        'binary' : boolean, default=False
            If True, all non zero counts are set to 1.
        'norm' : 'l1', 'l2' or None, optional, default='l2'
            Each output row will have unit norm, either:
            * 'l2': Sum of squares of vector elements is 1. The cosine
            similarity between two vectors is their dot product when l2 norm has
            been applied.
            * 'l1': Sum of absolute values of vector elements is 1.
            See :func:`utils.common.normalize`
        'use_idf' : boolean, default=True
            Enable inverse-document-frequency reweighting.
        'smooth_idf' : boolean, default=True
            Smooth idf weights by adding one to document frequencies, as if an
            extra document was seen containing every term in the collection
            exactly once. Prevents zero divisions.
        'sublinear_tf' : boolean (default=False)
            Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).

    """

    def __init__(self,
                 corpus: List[str] = None,
                 ids: List = None,
                 tokenizer: Tokenizer = None,
                 vocab: Vocabulary = None,
                 max_vocab: int = None,
                 max_doc_freq: Union[float, int] = 1.0,
                 min_doc_freq: int = 1,
                 tfidf_params: Dict = None,
                 **kwargs):
        super().__init__(ids=ids, **kwargs)
        self.corpus = corpus
        self.tokenizer = BaseTokenizer() if tokenizer is None else tokenizer
        self.vocab = vocab
        self.max_vocab = max_vocab
        self.max_doc_freq = max_doc_freq
        self.min_doc_freq = min_doc_freq
        self.tfidf_params = tfidf_params
        self.sequences = None
        self.count_matrix = None
        self.__tfidf_matrix = None

    @property
    def tfidf_matrix(self):
        """Return tf-idf matrix.
        """
        if self.__tfidf_matrix is None:
            params = {
                'tokenizer': self.tokenizer,
                'vocab': self.vocab,
                'max_doc_freq': self.max_doc_freq,
                'min_doc_freq': self.min_doc_freq,
                'max_features': self.max_vocab
            }
            self.tfidf_params = (params
                                 if self.tfidf_params is None
                                 else {**self.tfidf_params, **params})

            vectorizer = TfidfVectorizer(**self.tfidf_params)
            self.__tfidf_matrix = vectorizer.fit_transform(self.corpus)

        return self.__tfidf_matrix

    def _swap_text(self, id_map: Dict):
        new_corpus = self.corpus.copy()
        new_ids = self.ids.copy()
        for old_idx, raw_id in enumerate(self.ids):
            new_idx = id_map.get(raw_id, None)
            if new_idx is None:
                continue
            assert new_idx < len(self.corpus)
            new_corpus[new_idx] = self.corpus[old_idx]
            new_ids[new_idx] = raw_id
        self.corpus = new_corpus
        self.ids = new_ids

    def _build_text(self, id_map: Dict):
        """Build the text based on provided global id map
        """
        if self.corpus is None:
            return

        if (self.ids is not None) and (id_map is not None):
            self._swap_text(id_map)

        vectorizer = CountVectorizer(tokenizer=self.tokenizer, vocab=self.vocab,
                                     max_doc_freq=self.max_doc_freq, min_doc_freq=self.min_doc_freq,
                                     max_features=self.max_vocab, binary=False)
        self.sequences, self.count_matrix = vectorizer.fit_transform(self.corpus)
        self.vocab = Vocabulary(vectorizer.vocab.idx2tok, use_special_tokens=True)
        # Map tokens into integer ids
        for i, seq in enumerate(self.sequences):
            self.sequences[i] = self.vocab.to_idx(seq)

        # Reset other lazy-built properties (e.g. tfidf)
        self.__tfidf_matrix = None

    def build(self, id_map=None, **kwargs):
        """Build the model based on provided list of ordered ids

        Parameters
        ----------
        id_map: dict, optional
            A dictionary holds mapping from original ids to
            mapped integer indices of users/items.

        Returns
        -------
        text_modality: :obj:`<cornac.data.TextModality>`
            An object of type `TextModality`.

        """
        super().build(id_map=id_map)
        self._build_text(id_map)
        return self

    def batch_seq(self, batch_ids, max_length=None):
        """Return a numpy matrix of text sequences containing token ids
        with size=(len(batch_ids), max_length).

        Parameters
        ----------
        batch_ids: Union[List, numpy.array], required
            An array containing the ids of rows of text sequences to be returned.

        max_length: int, optional
            Cut-off length of returned sequences.
            If `None`, it will be inferred based on retrieved sequences.

        Returns
        -------
        batch_sequences: numpy.ndarray
            Batch of sequences with zero-padding at the end.

        """
        if self.sequences is None:
            raise ValueError('self.sequences is required but None!')

        if max_length is None:
            max_length = max(len(self.sequences[mapped_id]) for mapped_id in batch_ids)

        seq_mat = np.zeros((len(batch_ids), max_length), dtype='int')
        for i, mapped_id in enumerate(batch_ids):
            idx_seq = self.sequences[mapped_id][:max_length]
            for j, idx in enumerate(idx_seq):
                seq_mat[i, j] = idx

        return seq_mat

    @fallback_feature
    def batch_bow(self, batch_ids, binary=False, keep_sparse=False):
        """Return matrix of bag-of-words corresponding to provided batch_ids

        Parameters
        ----------
        batch_ids: array
            An array of ids to retrieve the corresponding features.

        binary: bool, default = False
            If `True`, the feature values will be converted into (0 or 1).

        keep_sparse: bool, default = False
            If `True`, the return feature matrix will be a `scipy.sparse.csr_matrix`.
            Otherwise, it will be a dense matrix.

        Returns
        -------
        batch_bow: numpy.ndarray
            Batch of bag-of-words representations corresponding to input `batch_ids`.

        """
        if self.count_matrix is None:
            raise ValueError('self.count_matrix is required but None!')

        bow_mat = self.count_matrix[batch_ids]
        if binary:
            bow_mat.data.fill(1)

        return bow_mat if keep_sparse else bow_mat.A

    def batch_tfidf(self, batch_ids, keep_sparse=False):
        """Return matrix of TF-IDF features corresponding to provided batch_ids

        Parameters
        ----------
        batch_ids: array
            An array of ids to retrieve the corresponding features.

        keep_sparse: bool, default = False
            If `True`, the return feature matrix will be a `scipy.sparse.csr_matrix`.
            Otherwise, it will be a dense matrix.

        Returns
        -------
        batch_tfidf: numpy.ndarray
            Batch of TF-IDF representations corresponding to input `batch_ids`.

        """
        tfidf_mat = self.tfidf_matrix[batch_ids]
        return tfidf_mat if keep_sparse else tfidf_mat.A

class ReviewModality(TextModality):
    """Review modality

    Parameters
    ----------
    data: List[tuple], required
        A triplet list of user, item, and review \
        e.g., data=[('user1', 'item1', 'review1'), ('user2', 'item2', 'review2)].

    group_by: 'user', 'item', or None, required, default = None
        Group mode. Whether reviews are grouped based on users, items, or not.

    tokenizer: Tokenizer, optional, default = None
        Tokenizer for text splitting. If None, the BaseTokenizer will be used.

    vocab: Vocabulary, optional, default = None
        Vocabulary of tokens. It contains mapping between tokens to their
        integer ids and vice versa.

    max_vocab: int, optional, default = None
        The maximum size of the vocabulary.
        If vocab is provided, this will be ignored.

    max_doc_freq: float in range [0.0, 1.0] or int, default=1.0
        When building the vocabulary ignore terms that have a document
        frequency strictly higher than the given threshold (corpus-specific
        stop words).
        If float, the value represents a proportion of documents, int for absolute counts.
        If `vocab` is not None, this will be ignored.

    min_doc_freq: float in range [0.0, 1.0] or int, default=1
        When building the vocabulary ignore terms that have a document
        frequency strictly lower than the given threshold. This value is also
        called cut-off in the literature.
        If float, the value represents a proportion of documents, int absolute counts.
        If `vocab` is not None, this will be ignored.

    tfidf_params: dict or None, optional, default=None
        If `None`, a default arguments of :obj:`<cornac.data.text.IfidfVectorizer>` will be used.
        List of parameters:

        'binary' : boolean, default=False
            If True, all non zero counts are set to 1.
        'norm' : 'l1', 'l2' or None, optional, default='l2'
            Each output row will have unit norm, either:
            * 'l2': Sum of squares of vector elements is 1. The cosine
            similarity between two vectors is their dot product when l2 norm has
            been applied.
            * 'l1': Sum of absolute values of vector elements is 1.
            See :func:`utils.common.normalize`
        'use_idf' : boolean, default=True
            Enable inverse-document-frequency reweighting.
        'smooth_idf' : boolean, default=True
            Smooth idf weights by adding one to document frequencies, as if an
            extra document was seen containing every term in the collection
            exactly once. Prevents zero divisions.
        'sublinear_tf' : boolean (default=False)
            Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).

    """
    def __init__(self,
                 data: List[tuple] = None,
                 group_by: str = None,
                 tokenizer: Tokenizer = None,
                 vocab: Vocabulary = None,
                 max_vocab: int = None,
                 max_doc_freq: Union[float, int] = 1.0,
                 min_doc_freq: int = 1,
                 tfidf_params: Dict = None,
                 **kwargs):
        super().__init__(
            tokenizer=tokenizer,
            vocab=vocab,
            max_vocab=max_vocab,
            max_doc_freq=max_doc_freq,
            min_doc_freq=min_doc_freq,
            tfidf_params=tfidf_params,
            **kwargs
        )
        self.raw_data = data
        if group_by not in ['user', 'item', None]:
            raise ValueError("group_by should be in ['user', 'item', None]")
        self.group_by = group_by

    def _build_corpus(self, uid_map, iid_map, dok_matrix):
        id_map = None
        corpus = None
        if self.group_by is None:
            self.user_review = OrderedDict()
            self.item_review = OrderedDict()

            reviews = OrderedDict()
            corpus = []
            for raw_uid, raw_iid, review in self.raw_data:
                user_idx = uid_map.get(raw_uid, None)
                item_idx = iid_map.get(raw_iid, None)
                if user_idx is None or item_idx is None or dok_matrix[user_idx, item_idx] == 0:
                    continue
                idx = len(reviews)
                reviews.setdefault(idx, review)
                user_dict = self.user_review.setdefault(user_idx, OrderedDict())
                user_dict[item_idx] = idx
                item_dict = self.item_review.setdefault(item_idx, OrderedDict())
                item_dict[user_idx] = idx
                corpus.append(review)
            self.reviews = reviews
        else:
            id_map = uid_map if self.group_by == 'user' else iid_map
            corpus = ['' for _ in range(len(id_map))]
            for raw_uid, raw_iid, review in self.raw_data:
                user_idx = uid_map.get(raw_uid, None)
                item_idx = iid_map.get(raw_iid, None)
                if user_idx is None or item_idx is None or dok_matrix[user_idx, item_idx] == 0:
                    continue
                _idx = user_idx if self.group_by == 'user' else item_idx
                corpus[_idx] = ' '.join([corpus[_idx], review.strip()])
        return corpus, id_map

    def build(self, uid_map=None, iid_map=None, dok_matrix=None, **kwargs):
        """Build the model based on provided list of ordered ids
        """
        if uid_map is None or iid_map is None or dok_matrix is None:
            raise ValueError('uid_map, iid_map, and dok_matrix are required')
        self.corpus, id_map = self._build_corpus(uid_map, iid_map, dok_matrix)
        super().build(id_map=id_map)

        return self

# dataset
# Copyright 2018 The Cornac Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

import copy
import os
import pickle
import warnings
from collections import Counter, OrderedDict, defaultdict

import numpy as np
from scipy.sparse import csc_matrix, csr_matrix, dok_matrix

class Dataset(object):
    """Training set contains preference matrix

    Parameters
    ----------
    num_users: int, required
        Number of users.

    num_items: int, required
        Number of items.

    uid_map: :obj:`OrderDict`, required
        The dictionary containing mapping from user original ids to mapped integer indices.

    iid_map: :obj:`OrderDict`, required
        The dictionary containing mapping from item original ids to mapped integer indices.

    uir_tuple: tuple, required
        Tuple of 3 numpy arrays (user_indices, item_indices, rating_values).

    timestamps: numpy.array, optional, default: None
        Array of timestamps corresponding to observations in `uir_tuple`.

    seed: int, optional, default: None
        Random seed for reproducing data sampling.

    Attributes
    ----------
    num_ratings: int
        Number of rating observations in the dataset.

    max_rating: float
        Maximum value among the rating observations.

    min_rating: float
        Minimum value among the rating observations.

    global_mean: float
        Average value over the rating observations.

    uir_tuple: tuple
        Tuple three numpy arrays (user_indices, item_indices, rating_values).

    timestamps: numpy.array
        Numpy array of timestamps corresponding to feedback in `uir_tuple`.
        This is only available when input data is in `UIRT` format.
    """

    def __init__(
        self,
        num_users,
        num_items,
        uid_map,
        iid_map,
        uir_tuple,
        timestamps=None,
        seed=None,
    ):
        self.num_users = num_users
        self.num_items = num_items
        self.uid_map = uid_map
        self.iid_map = iid_map
        self.uir_tuple = uir_tuple
        self.timestamps = timestamps
        self.seed = seed
        self.rng = get_rng(seed)

        (_, _, r_values) = uir_tuple
        self.num_ratings = len(r_values)
        self.max_rating = np.max(r_values)
        self.min_rating = np.min(r_values)
        self.global_mean = np.mean(r_values)

        self.__user_ids = None
        self.__item_ids = None
        self.__user_data = None
        self.__item_data = None
        self.__chrono_user_data = None
        self.__chrono_item_data = None
        self.__csr_matrix = None
        self.__csc_matrix = None
        self.__dok_matrix = None

        self.ignored_attrs = [
            "__user_ids",
            "__item_ids",
            "__user_data",
            "__item_data",
            "__chrono_user_data",
            "__chrono_item_data",
            "__csr_matrix",
            "__csc_matrix",
            "__dok_matrix",
        ]

    @property
    def user_ids(self):
        """Return the list of raw user ids"""
        if self.__user_ids is None:
            self.__user_ids = list(self.uid_map.keys())
        return self.__user_ids

    @property
    def item_ids(self):
        """Return the list of raw item ids"""
        if self.__item_ids is None:
            self.__item_ids = list(self.iid_map.keys())
        return self.__item_ids

    @property
    def user_data(self):
        """Data organized by user. A dictionary where keys are users,
        values are tuples of two lists (items, ratings) interacted by the corresponding users.
        """
        if self.__user_data is None:
            self.__user_data = defaultdict()
            for u, i, r in zip(*self.uir_tuple):
                u_data = self.__user_data.setdefault(u, ([], []))
                u_data[0].append(i)
                u_data[1].append(r)
        return self.__user_data

    @property
    def item_data(self):
        """Data organized by item. A dictionary where keys are items,
        values are tuples of two lists (users, ratings) interacted with the corresponding items.
        """
        if self.__item_data is None:
            self.__item_data = defaultdict()
            for u, i, r in zip(*self.uir_tuple):
                i_data = self.__item_data.setdefault(i, ([], []))
                i_data[0].append(u)
                i_data[1].append(r)
        return self.__item_data

    @property
    def chrono_user_data(self):
        """Data organized by user sorted chronologically (timestamps required).
        A dictionary where keys are users, values are tuples of three chronologically
        sorted lists (items, ratings, timestamps) interacted by the corresponding users.
        """
        if self.timestamps is None:
            raise ValueError("Timestamps are required but None!")

        if self.__chrono_user_data is None:
            self.__chrono_user_data = defaultdict()
            for u, i, r, t in zip(*self.uir_tuple, self.timestamps):
                u_data = self.__chrono_user_data.setdefault(u, ([], [], []))
                u_data[0].append(i)
                u_data[1].append(r)
                u_data[2].append(t)
            # sorting based on timestamps
            for user, (items, ratings, timestamps) in self.__chrono_user_data.items():
                sorted_idx = np.argsort(timestamps)
                sorted_items = [items[i] for i in sorted_idx]
                sorted_ratings = [ratings[i] for i in sorted_idx]
                sorted_timestamps = [timestamps[i] for i in sorted_idx]
                self.__chrono_user_data[user] = (
                    sorted_items,
                    sorted_ratings,
                    sorted_timestamps,
                )
        return self.__chrono_user_data

    @property
    def chrono_item_data(self):
        """Data organized by item sorted chronologically (timestamps required).
        A dictionary where keys are items, values are tuples of three chronologically
        sorted lists (users, ratings, timestamps) interacted with the corresponding items.
        """
        if self.timestamps is None:
            raise ValueError("Timestamps are required but None!")

        if self.__chrono_item_data is None:
            self.__chrono_item_data = defaultdict()
            for u, i, r, t in zip(*self.uir_tuple, self.timestamps):
                i_data = self.__chrono_item_data.setdefault(i, ([], [], []))
                i_data[0].append(u)
                i_data[1].append(r)
                i_data[2].append(t)
            # sorting based on timestamps
            for item, (users, ratings, timestamps) in self.__chrono_item_data.items():
                sorted_idx = np.argsort(timestamps)
                sorted_users = [users[i] for i in sorted_idx]
                sorted_ratings = [ratings[i] for i in sorted_idx]
                sorted_timestamps = [timestamps[i] for i in sorted_idx]
                self.__chrono_item_data[item] = (
                    sorted_users,
                    sorted_ratings,
                    sorted_timestamps,
                )
        return self.__chrono_item_data

    @property
    def matrix(self):
        """The user-item interaction matrix in CSR sparse format"""
        return self.csr_matrix

    @property
    def csr_matrix(self):
        """The user-item interaction matrix in CSR sparse format"""
        if self.__csr_matrix is None:
            (u_indices, i_indices, r_values) = self.uir_tuple
            self.__csr_matrix = csr_matrix(
                (r_values, (u_indices, i_indices)),
                shape=(self.num_users, self.num_items),
            )
        return self.__csr_matrix

    @property
    def csc_matrix(self):
        """The user-item interaction matrix in CSC sparse format"""
        if self.__csc_matrix is None:
            (u_indices, i_indices, r_values) = self.uir_tuple
            self.__csc_matrix = csc_matrix(
                (r_values, (u_indices, i_indices)),
                shape=(self.num_users, self.num_items),
            )
        return self.__csc_matrix

    @property
    def dok_matrix(self):
        """The user-item interaction matrix in DOK sparse format"""
        if self.__dok_matrix is None:
            self.__dok_matrix = dok_matrix((self.num_users, self.num_items), dtype="float")
            for u, i, r in zip(*self.uir_tuple):
                self.__dok_matrix[u, i] = r
        return self.__dok_matrix

    @classmethod
    def build(
        cls,
        data,
        fmt="UIR",
        global_uid_map=None,
        global_iid_map=None,
        seed=None,
        exclude_unknowns=False,
    ):
        """Constructing Dataset from given data of specific format.

        Parameters
        ----------
        data: array-like, required
            Data in the form of triplets (user, item, rating) for UIR format,
            or quadruplets (user, item, rating, timestamps) for UIRT format.

        fmt: str, default: 'UIR'
            Format of the input data. Currently, we are supporting:

            'UIR': User, Item, Rating
            'UIRT': User, Item, Rating, Timestamp

        global_uid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of users.

        global_iid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of items.

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        exclude_unknowns: bool, default: False
            Ignore unknown users and items.

        Returns
        -------
        res: :obj:`<cornac.data.Dataset>`
            Dataset object.

        """
        fmt = validate_format(fmt, ["UIR", "UIRT"])

        if global_uid_map is None:
            global_uid_map = OrderedDict()
        if global_iid_map is None:
            global_iid_map = OrderedDict()

        uid_map = OrderedDict()
        iid_map = OrderedDict()

        u_indices = []
        i_indices = []
        r_values = []
        valid_idx = []

        ui_set = set()  # avoid duplicate observations
        dup_count = 0

        for idx, (uid, iid, rating, *_) in enumerate(data):
            if exclude_unknowns and (uid not in global_uid_map or iid not in global_iid_map):
                continue

            if (uid, iid) in ui_set:
                dup_count += 1
                continue
            ui_set.add((uid, iid))

            uid_map[uid] = global_uid_map.setdefault(uid, len(global_uid_map))
            iid_map[iid] = global_iid_map.setdefault(iid, len(global_iid_map))

            u_indices.append(uid_map[uid])
            i_indices.append(iid_map[iid])
            r_values.append(float(rating))
            valid_idx.append(idx)

        if dup_count > 0:
            warnings.warn("%d duplicated observations are removed!" % dup_count)

        if len(ui_set) == 0:
            raise ValueError("data is empty after being filtered!")

        uir_tuple = (
            np.asarray(u_indices, dtype="int"),
            np.asarray(i_indices, dtype="int"),
            np.asarray(r_values, dtype="float"),
        )

        timestamps = np.fromiter((int(data[i][3]) for i in valid_idx), dtype="int") if fmt == "UIRT" else None

        dataset = cls(
            num_users=len(global_uid_map),
            num_items=len(global_iid_map),
            uid_map=global_uid_map,
            iid_map=global_iid_map,
            uir_tuple=uir_tuple,
            timestamps=timestamps,
            seed=seed,
        )

        return dataset

    @classmethod
    def from_uir(cls, data, seed=None):
        """Constructing Dataset from UIR (User, Item, Rating) triplet data.

        Parameters
        ----------
        data: array-like, shape: [n_examples, 3]
            Data in the form of triplets (user, item, rating)

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.Dataset>`
            Dataset object.

        """
        return cls.build(data, fmt="UIR", seed=seed)

    @classmethod
    def from_uirt(cls, data, seed=None):
        """Constructing Dataset from UIRT (User, Item, Rating, Timestamp)
        quadruplet data.

        Parameters
        ----------
        data: array-like, shape: [n_examples, 4]
            Data in the form of triplets (user, item, rating, timestamp)

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.Dataset>`
            Dataset object.

        """
        return cls.build(data, fmt="UIRT", seed=seed)

    def reset(self):
        """Reset the random number generator for reproducibility"""
        self.rng = get_rng(self.seed)
        return self

    def num_batches(self, batch_size):
        """Estimate number of batches per epoch"""
        return estimate_batches(len(self.uir_tuple[0]), batch_size)

    def num_user_batches(self, batch_size):
        """Estimate number of batches per epoch iterating over users"""
        return estimate_batches(self.num_users, batch_size)

    def num_item_batches(self, batch_size):
        """Estimate number of batches per epoch iterating over items"""
        return estimate_batches(self.num_items, batch_size)

    def idx_iter(self, idx_range, batch_size=1, shuffle=False):
        """Create an iterator over batch of indices

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional
            If True, orders of triplets will be randomized. If False, default orders kept

        Returns
        -------
        iterator : batch of indices (array of 'int')

        """
        indices = np.arange(idx_range)
        if shuffle:
            self.rng.shuffle(indices)

        n_batches = estimate_batches(len(indices), batch_size)
        for b in range(n_batches):
            start_offset = batch_size * b
            end_offset = batch_size * b + batch_size
            end_offset = min(end_offset, len(indices))
            batch_ids = indices[start_offset:end_offset]
            yield batch_ids

    def uir_iter(self, batch_size=1, shuffle=False, binary=False, num_zeros=0):
        """Create an iterator over data yielding batch of users, items, and rating values

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of triplets will be randomized. If `False`, default orders kept.

        binary: bool, optional, default: False
            If `True`, non-zero ratings will be turned into `1`, otherwise, values remain unchanged.

        num_zeros: int, optional, default = 0
            Number of unobserved ratings (zeros) to be added per user. This could be used
            for negative sampling. By default, no values are added.

        Returns
        -------
        iterator : batch of users (array of 'int'), batch of items (array of 'int'),
            batch of ratings (array of 'float')

        """
        for batch_ids in self.idx_iter(len(self.uir_tuple[0]), batch_size, shuffle):
            batch_users = self.uir_tuple[0][batch_ids]
            batch_items = self.uir_tuple[1][batch_ids]
            if binary:
                batch_ratings = np.ones_like(batch_items)
            else:
                batch_ratings = self.uir_tuple[2][batch_ids]

            if num_zeros > 0:
                repeated_users = batch_users.repeat(num_zeros)
                neg_items = np.empty_like(repeated_users)
                for i, u in enumerate(repeated_users):
                    j = self.rng.randint(0, self.num_items)
                    while self.dok_matrix[u, j] > 0:
                        j = self.rng.randint(0, self.num_items)
                    neg_items[i] = j
                batch_users = np.concatenate((batch_users, repeated_users))
                batch_items = np.concatenate((batch_items, neg_items))
                batch_ratings = np.concatenate((batch_ratings, np.zeros_like(neg_items)))

            yield batch_users, batch_items, batch_ratings

    def uij_iter(self, batch_size=1, shuffle=False, neg_sampling="uniform"):
        """Create an iterator over data yielding batch of users, positive items, and negative items

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of triplets will be randomized. If `False`, default orders kept.

        neg_sampling: str, optional, default: 'uniform'
            How negative item `j` will be sampled. Supported options: {`uniform`, `popularity`}.

        Returns
        -------
        iterator : batch of users (array of 'int'), batch of positive items (array of 'int'),
            batch of negative items (array of 'int')

        """
        if neg_sampling.lower() == "uniform":
            neg_population = np.arange(self.num_items)
        elif neg_sampling.lower() == "popularity":
            neg_population = self.uir_tuple[1]
        else:
            raise ValueError("Unsupported negative sampling option: {}".format(neg_sampling))

        for batch_ids in self.idx_iter(len(self.uir_tuple[0]), batch_size, shuffle):
            batch_users = self.uir_tuple[0][batch_ids]
            batch_pos_items = self.uir_tuple[1][batch_ids]
            batch_pos_ratings = self.uir_tuple[2][batch_ids]
            batch_neg_items = np.empty_like(batch_pos_items)
            for i, (user, pos_rating) in enumerate(zip(batch_users, batch_pos_ratings)):
                neg_item = self.rng.choice(neg_population)
                while self.dok_matrix[user, neg_item] >= pos_rating:
                    neg_item = self.rng.choice(neg_population)
                batch_neg_items[i] = neg_item
            yield batch_users, batch_pos_items, batch_neg_items

    def user_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over user indices

        Parameters
        ----------
        batch_size : int, optional, default = 1

        shuffle : bool, optional
            If True, orders of triplets will be randomized. If False, default orders kept

        Returns
        -------
        iterator : batch of user indices (array of 'int')
        """
        user_indices = np.fromiter(set(self.uir_tuple[0]), dtype="int")
        for batch_ids in self.idx_iter(len(user_indices), batch_size, shuffle):
            yield user_indices[batch_ids]

    def item_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over item indices

        Parameters
        ----------
        batch_size : int, optional, default = 1

        shuffle : bool, optional
            If True, orders of triplets will be randomized. If False, default orders kept

        Returns
        -------
        iterator : batch of item indices (array of 'int')
        """
        item_indices = np.fromiter(set(self.uir_tuple[1]), "int")
        for batch_ids in self.idx_iter(len(item_indices), batch_size, shuffle):
            yield item_indices[batch_ids]

    def add_modalities(self, **kwargs):
        self.user_feature = kwargs.get("user_feature", None)
        self.item_feature = kwargs.get("item_feature", None)
        self.user_text = kwargs.get("user_text", None)
        self.item_text = kwargs.get("item_text", None)
        self.user_image = kwargs.get("user_image", None)
        self.item_image = kwargs.get("item_image", None)
        self.user_graph = kwargs.get("user_graph", None)
        self.item_graph = kwargs.get("item_graph", None)
        self.sentiment = kwargs.get("sentiment", None)
        self.review_text = kwargs.get("review_text", None)

    def __deepcopy__(self, memo):
        cls = self.__class__
        result = cls.__new__(cls)
        for k, v in self.__dict__.items():
            if k in self.ignored_attrs:
                continue
            setattr(result, k, copy.deepcopy(v))
        return result

    def save(self, fpath):
        """Save a dataset to the filesystem.

        Parameters
        ----------
        fpath: str, required
            Path to a file for the dataset to be stored.

        """
        os.makedirs(os.path.dirname(fpath), exist_ok=True)
        dataset = copy.deepcopy(self)
        pickle.dump(dataset, open(fpath, "wb"), protocol=pickle.HIGHEST_PROTOCOL)

    @staticmethod
    def load(fpath):
        """Load a dataset from the filesystem.

        Parameters
        ----------
        fpath: str, required
            Path to a file where the dataset is stored.

        Returns
        -------
        self : object
        """
        dataset = pickle.load(open(fpath, "rb"))
        dataset.load_from = fpath  # for further loading
        return dataset


class BasketDataset(Dataset):
    """Training set contains history baskets

    Parameters
    ----------
    num_users: int, required
        Number of users.

    num_items: int, required
        Number of items.

    uid_map: :obj:`OrderDict`, required
        The dictionary containing mapping from user original ids to mapped integer indices.

    iid_map: :obj:`OrderDict`, required
        The dictionary containing mapping from item original ids to mapped integer indices.

    uir_tuple: tuple, required
        Tuple of 3 numpy arrays (user_indices, item_indices, rating_values).

    basket_indices: numpy.array, required
        Array of basket indices corresponding to observation in `uir_tuple`.

    timestamps: numpy.array, optional, default: None
        Numpy array of timestamps corresponding to feedback in `uir_tuple`.
        This is only available when input data is in `UBIT` and `UBITJson` formats.

    extra_data: numpy.array, optional, default: None
        Array of json object corresponding to observations in `uir_tuple`.

    seed: int, optional, default: None
        Random seed for reproducing data sampling.

    Attributes
    ----------
    ubi_tuple: tuple
        Tuple (user_indices, baskets).

    timestamps: numpy.array
        Numpy array of timestamps corresponding to feedback in `ubi_tuple`.
        This is only available when input data is in `UTB` format.
    """

    def __init__(
        self,
        num_users,
        num_baskets,
        num_items,
        uid_map,
        bid_map,
        iid_map,
        uir_tuple,
        basket_indices=None,
        timestamps=None,
        extra_data=None,
        seed=None,
    ):
        super().__init__(
            num_users=num_users,
            num_items=num_items,
            uid_map=uid_map,
            iid_map=iid_map,
            uir_tuple=uir_tuple,
            timestamps=timestamps,
            seed=seed,
        )
        self.num_baskets = num_baskets
        self.bid_map = bid_map
        self.basket_indices = basket_indices
        self.extra_data = extra_data
        basket_sizes = list(Counter(basket_indices).values())
        self.max_basket_size = np.max(basket_sizes)
        self.min_basket_size = np.min(basket_sizes)
        self.avg_basket_size = np.mean(basket_sizes)

        self.__baskets = None
        self.__basket_ids = None
        self.__user_basket_data = None
        self.__chrono_user_basket_data = None

    @property
    def basket_ids(self):
        """Return the list of raw basket ids"""
        if self.__basket_ids is None:
            self.__basket_ids = list(self.bid_map.keys())
        return self.__basket_ids

    @property
    def baskets(self):
        """A dictionary to store indices where basket ID appears in the data."""
        if self.__baskets is None:
            self.__baskets = defaultdict(list)
            for idx, bid in enumerate(self.basket_indices):
                self.__baskets[bid].append(idx)
        return self.__baskets

    @property
    def user_basket_data(self):
        """Data organized by user. A dictionary where keys are users,
        values are list of baskets purchased by corresponding users.
        """
        if self.__user_basket_data is None:
            self.__user_basket_data = defaultdict(list)
            for bid, ids in self.baskets.items():
                u = self.uir_tuple[0][ids[0]]
                self.__user_basket_data[u].append(bid)
        return self.__user_basket_data

    @property
    def chrono_user_basket_data(self):
        """Data organized by user sorted chronologically (timestamps required).
        A dictionary where keys are users, values are tuples of three chronologically
        sorted lists (baskets, timestamps) interacted by the corresponding users.
        """
        if self.__chrono_user_basket_data is None:
            assert self.timestamps is not None  # we need timestamps

            basket_timestamps = [self.timestamps[ids[0]] for ids in self.baskets.values()]  # one-off

            self.__chrono_user_basket_data = defaultdict(lambda: ([], []))
            for (bid, ids), t in zip(self.baskets.items(), basket_timestamps):
                u = self.uir_tuple[0][ids[0]]
                self.__chrono_user_basket_data[u][0].append(bid)
                self.__chrono_user_basket_data[u][1].append(t)

            # sorting based on timestamps
            for user, (baskets, timestamps) in self.__chrono_user_basket_data.items():
                sorted_idx = np.argsort(timestamps)
                sorted_baskets = [baskets[i] for i in sorted_idx]
                sorted_timestamps = [timestamps[i] for i in sorted_idx]
                self.__chrono_user_basket_data[user] = (
                    sorted_baskets,
                    sorted_timestamps,
                )

        return self.__chrono_user_basket_data

    @classmethod
    def build(
        cls,
        data,
        fmt="UBI",
        global_uid_map=None,
        global_bid_map=None,
        global_iid_map=None,
        seed=None,
        exclude_unknowns=False,
    ):
        """Constructing Dataset from given data of specific format.

        Parameters
        ----------
        data: list, required
            Data in the form of tuple (user, basket) for UB format,
            or tuple (user, timestamps, basket) for UTB format.

        fmt: str, default: 'UBI'
            Format of the input data. Currently, we are supporting:

            'UBI': User, Basket_ID, Item
            'UBIT': User, Basket_ID, Item, Timestamp
            'UBITJson': User, Basket_ID, Item, Timestamp, Extra data in Json format

        global_uid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of users.

        global_bid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of baskets.

        global_iid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of items.

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        exclude_unknowns: bool, default: False
            Ignore unknown users and items.

        Returns
        -------
        res: :obj:`<cornac.data.BasketDataset>`
            BasketDataset object.

        """
        fmt = validate_format(fmt, ["UBI", "UBIT", "UBITJson"])

        if global_uid_map is None:
            global_uid_map = OrderedDict()
        if global_bid_map is None:
            global_bid_map = OrderedDict()
        if global_iid_map is None:
            global_iid_map = OrderedDict()

        u_indices = []
        b_indices = []
        i_indices = []
        valid_idx = []
        extra_data = []
        for idx, (uid, bid, iid, *_) in enumerate(data):
            if exclude_unknowns and (iid not in global_iid_map):
                continue

            global_uid_map.setdefault(uid, len(global_uid_map))
            global_bid_map.setdefault(bid, len(global_bid_map))
            global_iid_map.setdefault(iid, len(global_iid_map))

            u_indices.append(global_uid_map[uid])
            b_indices.append(global_bid_map[bid])
            i_indices.append(global_iid_map[iid])
            valid_idx.append(idx)

        uir_tuple = (
            np.asarray(u_indices, dtype="int"),
            np.asarray(i_indices, dtype="int"),
            np.ones(len(u_indices), dtype="float"),
        )

        basket_indices = np.asarray(b_indices, dtype="int")

        timestamps = (
            np.fromiter((int(data[i][3]) for i in valid_idx), dtype="int") if fmt in ["UBIT", "UBITJson"] else None
        )

        extra_data = [data[i][4] for i in valid_idx] if fmt == "UBITJson" else None

        dataset = cls(
            num_users=len(global_uid_map),
            num_baskets=len(global_bid_map),
            num_items=len(global_iid_map),
            uid_map=global_uid_map,
            bid_map=global_bid_map,
            iid_map=global_iid_map,
            uir_tuple=uir_tuple,
            basket_indices=basket_indices,
            timestamps=timestamps,
            extra_data=extra_data,
            seed=seed,
        )

        return dataset

    @classmethod
    def from_ubi(cls, data, seed=None):
        """Constructing Dataset from UBI (User, Basket, Item) triples data.

        Parameters
        ----------
        data: list
            Data in the form of tuples (user, basket, item).

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.BasketDataset>`
            BasketDataset object.

        """
        return cls.build(data, fmt="UBI", seed=seed)

    @classmethod
    def from_ubit(cls, data, seed=None):
        """Constructing Dataset from UBIT format (User, Basket, Item, Timestamp)

        Parameters
        ----------
        data: tuple
            Data in the form of quadruples (user, basket, item, timestamp)

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.BasketDataset>`
            BasketDataset object.

        """
        return cls.build(data, fmt="UBIT", seed=seed)

    @classmethod
    def from_ubitjson(cls, data, seed=None):
        """Constructing Dataset from UBITJson format (User, Basket, Item, Timestamp, Json)

        Parameters
        ----------
        data: tuple
            Data in the form of tuples (user, basket, item, timestamp, json)

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.BasketDataset>`
            BasketDataset object.

        """
        return cls.build(data, fmt="UBITJson", seed=seed)

    def ub_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over data yielding batch of users and batch of baskets

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of users will be randomized. If `False`, default orders kept.

        Returns
        -------
        iterator : batch of user indices, batch of baskets corresponding to user indices

        """
        for batch_users in self.user_iter(batch_size, shuffle):
            batch_baskets = [self.user_basket_data[uid] for uid in batch_users]
            yield batch_users, batch_baskets

    def ubi_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over data yielding batch of users, basket ids, and batch of the corresponding items

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of users will be randomized. If `False`, default orders kept.

        Returns
        -------
        iterator : batch of user indices, batch of baskets corresponding to user indices, and batch of items correponding to baskets

        """
        _, item_indices, _ = self.uir_tuple
        for batch_users, batch_baskets in self.ub_iter(batch_size, shuffle):
            batch_basket_items = [
                [item_indices[self.baskets[bid]] for bid in user_baskets] for user_baskets in batch_baskets
            ]
            yield batch_users, batch_baskets, batch_basket_items

    def basket_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over data yielding batch of basket indices

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of triplets will be randomized. If `False`, default orders kept.

        Returns
        -------
        iterator : batch of basket indices (array of 'int')

        """
        basket_indices = np.fromiter(set(self.baskets.keys()), dtype="int")
        for batch_ids in self.idx_iter(len(basket_indices), batch_size, shuffle):
            yield basket_indices[batch_ids]


class SequentialDataset(Dataset):
    """Training set contains history sessions

    Parameters
    ----------
    num_users: int, required
        Number of users.

    num_items: int, required
        Number of items.

    uid_map: :obj:`OrderDict`, required
        The dictionary containing mapping from user original ids to mapped integer indices.

    iid_map: :obj:`OrderDict`, required
        The dictionary containing mapping from item original ids to mapped integer indices.

    uir_tuple: tuple, required
        Tuple of 3 numpy arrays (user_indices, item_indices, rating_values).

    session_ids: numpy.array, required
        Array of session indices corresponding to observation in `uir_tuple`.

    timestamps: numpy.array, optional, default: None
        Numpy array of timestamps corresponding to feedback in `uir_tuple`.
        This is only available when input data is in `SIT`, `USIT`, SITJson`, and `USITJson` formats.

    extra_data: numpy.array, optional, default: None
        Array of json object corresponding to observations in `uir_tuple`.

    seed: int, optional, default: None
        Random seed for reproducing data sampling.

    Attributes
    ----------
    timestamps: numpy.array
        Numpy array of timestamps corresponding to feedback in `ubi_tuple`.
        This is only available when input data is in `UTB` format.
    """

    def __init__(
        self,
        num_users,
        num_sessions,
        num_items,
        uid_map,
        sid_map,
        iid_map,
        uir_tuple,
        session_indices=None,
        timestamps=None,
        extra_data=None,
        seed=None,
    ):
        super().__init__(
            num_users=num_users,
            num_items=num_items,
            uid_map=uid_map,
            iid_map=iid_map,
            uir_tuple=uir_tuple,
            timestamps=timestamps,
            seed=seed,
        )
        self.num_sessions = num_sessions
        self.sid_map = sid_map
        self.session_indices = session_indices
        self.extra_data = extra_data
        session_sizes = list(Counter(session_indices).values())
        self.max_session_size = np.max(session_sizes)
        self.min_session_size = np.min(session_sizes)
        self.avg_session_size = np.mean(session_sizes)

        self.__sessions = None
        self.__session_ids = None
        self.__user_session_data = None
        self.__chrono_user_session_data = None

    @property
    def session_ids(self):
        """Return the list of raw session ids"""
        if self.__session_ids is None:
            self.__session_ids = list(self.sid_map.keys())
        return self.__session_ids

    @property
    def sessions(self):
        """A dictionary to store indices where session ID appears in the data."""
        if self.__sessions is None:
            self.__sessions = OrderedDict()
            for idx, sid in enumerate(self.session_indices):
                self.__sessions.setdefault(sid, [])
                self.__sessions[sid].append(idx)
        return self.__sessions

    @property
    def user_session_data(self):
        """Data organized by user. A dictionary where keys are users,
        values are list of sessions purchased by corresponding users.
        """
        if self.__user_session_data is None:
            self.__user_session_data = defaultdict(list)
            for sid, ids in self.sessions.items():
                u = self.uir_tuple[0][ids[0]]
                self.__user_session_data[u].append(sid)
        return self.__user_session_data

    @property
    def chrono_user_session_data(self):
        """Data organized by user sorted chronologically (timestamps required).
        A dictionary where keys are users, values are tuples of three chronologically
        sorted lists (sessions, timestamps) interacted by the corresponding users.
        """
        if self.__chrono_user_session_data is None:
            assert self.timestamps is not None  # we need timestamps

            session_timestamps = [self.timestamps[ids[0]] for ids in self.sessions.values()]  # one-off

            self.__chrono_user_session_data = defaultdict(lambda: ([], []))
            for (sid, ids), t in zip(self.sessions.items(), session_timestamps):
                u = self.uir_tuple[0][ids[0]]
                self.__chrono_user_session_data[u][0].append(sid)
                self.__chrono_user_session_data[u][1].append(t)

            # sorting based on timestamps
            for user, (sessions, timestamps) in self.__chrono_user_session_data.items():
                sorted_idx = np.argsort(timestamps)
                sorted_sessions = [sessions[i] for i in sorted_idx]
                sorted_timestamps = [timestamps[i] for i in sorted_idx]
                self.__chrono_user_session_data[user] = (
                    sorted_sessions,
                    sorted_timestamps,
                )

        return self.__chrono_user_session_data

    @classmethod
    def build(
        cls,
        data,
        fmt="SIT",
        global_uid_map=None,
        global_sid_map=None,
        global_iid_map=None,
        seed=None,
        exclude_unknowns=False,
    ):
        """Constructing Dataset from given data of specific format.

        Parameters
        ----------
        data: list, required
            Data in the form of tuple (user, session) for UB format,
            or tuple (user, timestamps, session) for UTB format.

        fmt: str, default: 'SIT'
            Format of the input data. Currently, we are supporting:

            'SIT': Session_ID, Item, Timestamp
            'USIT': User, Session_ID, Item, Timestamp
            'SITJson': Session_ID, Item, Timestamp, Extra data in Json format
            'USITJson': User, Session_ID, Item, Timestamp, Extra data in Json format

        global_uid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of users.

        global_sid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of sessions.

        global_iid_map: :obj:`defaultdict`, optional, default: None
            The dictionary containing global mapping from original ids to mapped ids of items.

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        exclude_unknowns: bool, default: False
            Ignore unknown users and items.

        Returns
        -------
        res: :obj:`<cornac.data.SequentialDataset>`
            SequentialDataset object.

        """
        fmt = validate_format(fmt, ["SIT", "USIT", "SITJson", "USITJson"])

        if global_uid_map is None:
            global_uid_map = OrderedDict()
        if global_sid_map is None:
            global_sid_map = OrderedDict()
        if global_iid_map is None:
            global_iid_map = OrderedDict()

        u_indices = []
        s_indices = []
        i_indices = []
        valid_idx = []
        extra_data = []
        for idx, tup in enumerate(data):
            uid, sid, iid, *_ = tup if fmt in ["USIT", "USITJson"] else [None] + list(tup)
            if exclude_unknowns and (iid not in global_iid_map):
                continue
            global_uid_map.setdefault(uid, len(global_uid_map))
            global_sid_map.setdefault(sid, len(global_sid_map))
            global_iid_map.setdefault(iid, len(global_iid_map))

            u_indices.append(global_uid_map[uid])
            s_indices.append(global_sid_map[sid])
            i_indices.append(global_iid_map[iid])
            valid_idx.append(idx)

        uir_tuple = (
            np.asarray(u_indices, dtype="int"),
            np.asarray(i_indices, dtype="int"),
            np.ones(len(u_indices), dtype="float"),
        )

        session_indices = np.asarray(s_indices, dtype="int")

        ts_pos = 3 if fmt in ["USIT", "USITJson"] else 2
        timestamps = (
            np.fromiter((int(data[i][ts_pos]) for i in valid_idx), dtype="int")
            if fmt in ["SIT", "SITJson", "USIT", "USITJson"]
            else None
        )

        extra_pos = ts_pos + 1
        extra_data = [data[i][extra_pos] for i in valid_idx] if fmt in ["SITJson", "USITJson"] else None

        dataset = cls(
            num_users=len(global_uid_map),
            num_sessions=len(set(session_indices)),
            num_items=len(global_iid_map),
            uid_map=global_uid_map,
            sid_map=global_sid_map,
            iid_map=global_iid_map,
            uir_tuple=uir_tuple,
            session_indices=session_indices,
            timestamps=timestamps,
            extra_data=extra_data,
            seed=seed,
        )

        return dataset

    @classmethod
    def from_sit(cls, data, seed=None):
        """Constructing Dataset from SIT (Session, Item, Timestamp) triples data.

        Parameters
        ----------
        data: list
            Data in the form of tuples (session, item, timestamp).

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.SequentialDataset>`
            SequentialDataset object.

        """
        return cls.build(data, fmt="SIT", seed=seed)

    @classmethod
    def from_usit(cls, data, seed=None):
        """Constructing Dataset from USIT format (User, Session, Item, Timestamp)

        Parameters
        ----------
        data: tuple
            Data in the form of quadruples (user, session, item, timestamp)

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.SequentialDataset>`
            SequentialDataset object.

        """
        return cls.build(data, fmt="USIT", seed=seed)

    @classmethod
    def from_sitjson(cls, data, seed=None):
        """Constructing Dataset from SITJson format (Session, Item, Timestamp, Json)

        Parameters
        ----------
        data: tuple
            Data in the form of tuples (session, item, timestamp, json)

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.SequentialDataset>`
            SequentialDataset object.

        """
        return cls.build(data, fmt="SITJson", seed=seed)

    @classmethod
    def from_usitjson(cls, data, seed=None):
        """Constructing Dataset from USITJson format (User, Session, Item, Timestamp, Json)

        Parameters
        ----------
        data: tuple
            Data in the form of tuples (user, session, item, timestamp, json)

        seed: int, optional, default: None
            Random seed for reproducing data sampling.

        Returns
        -------
        res: :obj:`<cornac.data.SequentialDataset>`
            SequentialDataset object.

        """
        return cls.build(data, fmt="USITJson", seed=seed)

    def num_batches(self, batch_size):
        """Estimate number of batches per epoch"""
        return estimate_batches(len(self.sessions), batch_size)

    def session_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over session indices

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of session_ids will be randomized. If `False`, default orders kept.

        Returns
        -------
        iterator : batch of session indices (array of 'int')

        """
        session_indices = np.array(list(self.sessions.keys()))
        for batch_ids in self.idx_iter(len(session_indices), batch_size, shuffle):
            batch_session_indices = session_indices[batch_ids]
            yield batch_session_indices

    def s_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over data yielding batch of sessions

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of sessions will be randomized. If `False`, default orders kept.

        Returns
        -------
        iterator : batch of session indices, batch of indices corresponding to session indices

        """
        for batch_session_ids in self.session_iter(batch_size, shuffle):
            batch_mapped_ids = [self.sessions[sid] for sid in batch_session_ids]
            yield batch_session_ids, batch_mapped_ids

    def si_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over data yielding batch of session indices, batch of mapped ids, and batch of sessions' items

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of triplets will be randomized. If `False`, default orders kept.

        Returns
        -------
        iterator : batch of session indices, batch mapped ids, batch of sessions' items (list of list)

        """
        for batch_session_indices, batch_mapped_ids in self.s_iter(batch_size, shuffle):
            batch_session_items = [[self.uir_tuple[1][i] for i in ids] for ids in batch_mapped_ids]
            yield batch_session_indices, batch_mapped_ids, batch_session_items

    def usi_iter(self, batch_size=1, shuffle=False):
        """Create an iterator over data yielding batch of user indices, batch of session indices, batch of mapped ids, and batch of sessions' items

        Parameters
        ----------
        batch_size: int, optional, default = 1

        shuffle: bool, optional, default: False
            If `True`, orders of triplets will be randomized. If `False`, default orders kept.

        Returns
        -------
        iterator : batch of user indices, batch of session indices (list of list), batch mapped ids (list of list of list), batch of sessions' items (list of list of list)

        """
        for user_indices in self.user_iter(batch_size, shuffle):
            batch_sids = [[sid for sid in self.user_session_data[uid]] for uid in user_indices]
            batch_mapped_ids = [[self.sessions[sid] for sid in self.user_session_data[uid]] for uid in user_indices]
            batch_session_items = [[[self.uir_tuple[1][i] for i in ids] for ids in u_batch_mapped_ids] for u_batch_mapped_ids in batch_mapped_ids]
            yield user_indices, batch_sids, batch_mapped_ids, batch_session_items

# RatingMetric
# Copyright 2018 The Cornac Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

import numpy as np


class RatingMetric:
    """Rating Metric.

    Attributes
    ----------
    name: string,
        Name of the measure.

    type: string, value: 'rating'
        Type of the metric, e.g., "ranking", "rating".

    """

    def __init__(self, name=None, higher_better=False):
        self.type = 'rating'
        self.name = name
        self.higher_better = higher_better

    def compute(self, **kwargs):
        raise NotImplementedError()


class MAE(RatingMetric):
    """Mean Absolute Error.

    Attributes
    ----------
    name: string, value: 'MAE'
        Name of the measure.

    """

    def __init__(self):
        RatingMetric.__init__(self, name='MAE')

    def compute(self, gt_ratings, pd_ratings, weights=None, **kwargs):
        """Compute Mean Absolute Error.

        Parameters
        ----------
        gt_ratings: Numpy array
            Ground-truth rating values.

        pd_ratings: Numpy array
            Predicted rating values.

        weights: Numpy array, optional, default: None
            Weights for rating values.

        **kwargs: For compatibility

        Returns
        -------
        mae: A scalar.
            Mean Absolute Error.

        """
        mae = np.average(np.abs(gt_ratings - pd_ratings), axis=0, weights=weights)
        return mae


class MSE(RatingMetric):
    """Mean Squared Error.

    Attributes
    ----------
    name: string, value: 'MSE'
        Name of the measure.

    """

    def __init__(self):
        RatingMetric.__init__(self, name='MSE')

    def compute(self, gt_ratings, pd_ratings, weights=None, **kwargs):
        """Compute Mean Squared Error.

        Parameters
        ----------
        gt_ratings: Numpy array
            Ground-truth rating values.

        pd_ratings: Numpy array
            Predicted rating values.

        weights: Numpy array, optional, default: None
            Weights for rating values.

        **kwargs: For compatibility

        Returns
        -------
        mse: A scalar.
            Mean Squared Error.

        """
        mse = np.average((gt_ratings - pd_ratings) ** 2, axis=0, weights=weights)
        return mse


class RMSE(RatingMetric):
    """Root Mean Squared Error.

    Attributes
    ----------
    name: string, value: 'RMSE'
        Name of the measure.

    """

    def __init__(self):
        RatingMetric.__init__(self, name='RMSE')

    def compute(self, gt_ratings, pd_ratings, weights=None, **kwargs):
        """Compute Root Mean Squared Error.

        Parameters
        ----------
        gt_ratings: Numpy array
            Ground-truth rating values.

        pd_ratings: Numpy array
            Predicted rating values.

        weights: Numpy array, optional, default: None
            Weights for rating values.

        **kwargs: For compatibility

        Returns
        -------
        rmse: A scalar.
            Root Mean Squared Error.

        """
        # print('in rmse metric compute')
        # print('type gt rating: ', type(gt_ratings))
        # print('type pd rating: ', type(pd_ratings))
        # print('len gt ratings: ', len(gt_ratings))
        # print('len pd ratings: ', len(pd_ratings))
        # try:
        #     print("10 giá trị đầu tiên của gt_ratings:", gt_ratings[:10])
        #     print("10 giá trị đầu tiên của pd_ratings:", pd_ratings[:10])
        # except Exception as e:
        #     print('error when show; ',e)
        # ipdb.set_trace()
        mse = np.average((gt_ratings - pd_ratings) ** 2, axis=0, weights=weights)
        return np.sqrt(mse)

# Ranking Metric
# Copyright 2018 The Cornac Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================

import numpy as np
from scipy.stats import rankdata


class RankingMetric:
    """Ranking Metric.

    Attributes
    ----------
    type: string, value: 'ranking'
        Type of the metric, e.g., "ranking", "rating".

    name: string, default: None
        Name of the measure.

    k: int or list, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    """

    def __init__(self, name=None, k=-1, higher_better=True):
        assert hasattr(k, "__len__") or k == -1 or k > 0

        self.type = "ranking"
        self.name = name
        self.k = k
        self.higher_better = higher_better

    def compute(self, **kwargs):
        raise NotImplementedError()


class NDCG(RankingMetric):
    """Normalized Discount Cumulative Gain.

    Parameters
    ----------
    k: int or list, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    References
    ----------
    https://en.wikipedia.org/wiki/Discounted_cumulative_gain

    """

    def __init__(self, k=-1):
        RankingMetric.__init__(self, name="NDCG@{}".format(k), k=k)

    @staticmethod
    def dcg_score(gt_pos, pd_rank, k=-1):
        """Compute Discounted Cumulative Gain score.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        k: int, optional, default: -1 (all)
            The number of items in the top@k list.
            If None, all items will be considered.

        Returns
        -------
        dcg: A scalar
            Discounted Cumulative Gain score.

        """
        if k > 0:
            truncated_pd_rank = pd_rank[:k]
        else:
            truncated_pd_rank = pd_rank

        ranked_scores = np.in1d(truncated_pd_rank, gt_pos).astype(int)
        gain = 2**ranked_scores - 1
        discounts = np.log2(np.arange(len(ranked_scores)) + 2)

        return np.sum(gain / discounts)

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute Normalized Discounted Cumulative Gain score.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        ndcg: A scalar
            Normalized Discounted Cumulative Gain score.

        """
        dcg = self.dcg_score(gt_pos, pd_rank, self.k)
        idcg = self.dcg_score(gt_pos, gt_pos, self.k)
        ndcg = dcg / idcg

        return ndcg


class NCRR(RankingMetric):
    """Normalized Cumulative Reciprocal Rank.

    Parameters
    ----------
    k: int or list, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    """

    def __init__(self, k=-1):
        RankingMetric.__init__(self, name="NCRR@{}".format(k), k=k)

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute Normalized Cumulative Reciprocal Rank score.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        ncrr: A scalar
            Normalized Cumulative Reciprocal Rank score.

        """
        if self.k > 0:
            truncated_pd_rank = pd_rank[: self.k]
        else:
            truncated_pd_rank = pd_rank

        # Compute CRR
        rec_rank = np.where(np.in1d(truncated_pd_rank, gt_pos))[0]
        if len(rec_rank) == 0:
            return 0.0
        rec_rank = rec_rank + 1  # +1 because indices starts from 0 in python
        crr = np.sum(1.0 / rec_rank)

        # Compute Ideal CRR
        max_nb_pos = min(len(gt_pos), len(truncated_pd_rank))
        ideal_rank = np.arange(max_nb_pos)
        ideal_rank = ideal_rank + 1  # +1 because indices starts from 0 in python
        icrr = np.sum(1.0 / ideal_rank)

        # Compute nDCG
        ncrr_i = crr / icrr

        return ncrr_i


class MRR(RankingMetric):
    """Mean Reciprocal Rank.

    References
    ----------
    https://en.wikipedia.org/wiki/Mean_reciprocal_rank
    """

    def __init__(self):
        RankingMetric.__init__(self, name="MRR")

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute Mean Reciprocal Rank score.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        mrr: A scalar
            Mean Reciprocal Rank score.

        """
        matched_items = np.nonzero(np.in1d(pd_rank, gt_pos))[0]

        if len(matched_items) == 0:
            raise ValueError(
                "No matched between ground-truth items and recommendations"
            )

        mrr = np.divide(
            1, (matched_items[0] + 1)
        )  # +1 because indices start from 0 in python
        return mrr


class MeasureAtK(RankingMetric):
    """Measure at K.

    Attributes
    ----------
    k: int or list, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    """

    def __init__(self, name=None, k=-1):
        RankingMetric.__init__(self, name, k)

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute TP, TP+FN, and TP+FP.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        tp: A scalar
            True positive.

        tp_fn: A scalar
            True positive + false negative.

        tp_fp: A scalar
            True positive + false positive.

        """
        if self.k > 0:
            truncated_pd_rank = pd_rank[: self.k]
        else:
            truncated_pd_rank = pd_rank

        tp = np.sum(np.in1d(truncated_pd_rank, gt_pos))
        tp_fn = len(gt_pos)
        tp_fp = self.k if self.k > 0 else len(truncated_pd_rank)

        return tp, tp_fn, tp_fp


class HitRatio(MeasureAtK):
    """Hit Ratio.

    Parameters
    ----------
    k: int, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    """

    def __init__(self, k=-1):
        super().__init__(name="HitRatio@{}".format(k), k=k)

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute Hit Ratio.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        res: A scalar
            Hit Ratio score (1.0 ground truth item(s) appear in top-k, 0 otherwise).

        """
        tp, *_ = MeasureAtK.compute(self, gt_pos, pd_rank, **kwargs)

        return 1.0 if tp > 0 else 0.0


class Precision(MeasureAtK):
    """Precision@K.

    Parameters
    ----------
    k: int or list, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    """

    def __init__(self, k=-1):
        super().__init__(name="Precision@{}".format(k), k=k)

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute Precision score.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        res: A scalar
            Precision score.

        """
        tp, _, tp_fp = MeasureAtK.compute(self, gt_pos, pd_rank, **kwargs)
        return tp / tp_fp


class Recall(MeasureAtK):
    """Recall@K.

    Parameters
    ----------
    k: int or list, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    """

    def __init__(self, k=-1):
        super().__init__(name="Recall@{}".format(k), k=k)

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute Recall score.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        res: A scalar
            Recall score.

        """
        tp, tp_fn, _ = MeasureAtK.compute(self, gt_pos, pd_rank, **kwargs)
        return tp / tp_fn


class FMeasure(MeasureAtK):
    """F-measure@K.

    Parameters
    ----------
    k: int or list, optional, default: -1 (all)
        The number of items in the top@k list.
        If None, all items will be considered.

    """

    def __init__(self, k=-1):
        super().__init__(name="F1@{}".format(k), k=k)

    def compute(self, gt_pos, pd_rank, **kwargs):
        """Compute F-Measure.

        Parameters
        ----------
        gt_pos: Numpy array
            Vector of positive items.

        pd_rank: Numpy array
            Item ranking prediction.

        **kwargs: For compatibility

        Returns
        -------
        res: A scalar
            F-Measure score.

        """
        tp, tp_fn, tp_fp = MeasureAtK.compute(self, gt_pos, pd_rank, **kwargs)

        prec = tp / tp_fp
        rec = tp / tp_fn
        if (prec + rec) > 0:
            f1 = 2 * (prec * rec) / (prec + rec)
        else:
            f1 = 0

        return f1


class AUC(RankingMetric):
    """Area Under the ROC Curve (AUC).

    References
    ----------
    https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf

    """

    def __init__(self):
        RankingMetric.__init__(self, name="AUC")

    def compute(self, item_indices, pd_scores, gt_pos, gt_neg=None, **kwargs):
        """Compute Area Under the ROC Curve (AUC).

        Parameters
        ----------
        item_indices: Numpy array
            Items being considered for evaluation.

        pd_scores: Numpy array
            Prediction scores for items in item_indices.

        gt_pos: Numpy array
            Vector of positive items.

        gt_neg: Numpy array, optional
            Vector of negative items.
            If None, negation of gt_pos will be used.

        **kwargs: For compatibility

        Returns
        -------
        res: A scalar
            AUC score.

        """

        gt_pos_mask = np.in1d(item_indices, gt_pos)
        gt_neg_mask = (
            np.logical_not(gt_pos_mask)
            if gt_neg is None
            else np.in1d(item_indices, gt_neg)
        )

        pos_scores = pd_scores[gt_pos_mask]
        neg_scores = pd_scores[gt_neg_mask]
        ui_scores = np.repeat(pos_scores, len(neg_scores))
        uj_scores = np.tile(neg_scores, len(pos_scores))

        return (ui_scores > uj_scores).sum() / len(uj_scores)


class MAP(RankingMetric):
    """Mean Average Precision (MAP).

    References
    ----------
    https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision

    """

    def __init__(self):
        RankingMetric.__init__(self, name="MAP")

    def compute(self, item_indices, pd_scores, gt_pos, **kwargs):
        """Compute Average Precision.

        Parameters
        ----------
        item_indices: Numpy array
            Items being considered for evaluation.

        pd_scores: Numpy array
            Prediction scores for items.

        gt_pos: Numpy array
            Vector of positive items.

        **kwargs: For compatibility

        Returns
        -------
        res: A scalar
            AP score.

        """
        relevant = np.in1d(item_indices, gt_pos)
        rank = rankdata(-pd_scores, "max")[relevant]
        L = rankdata(-pd_scores[relevant], "max")
        ans = (L / rank).mean()

        return ans

# Result
class Result:
    """
    Result Class for a single model

    Parameters
    ----------
    model_name: string, required
        The name of the recommender model.

    metric_avg_results: :obj:`OrderedDict`, required
        A dictionary containing the average result per-metric.

    metric_user_results: :obj:`OrderedDict`, required
        A dictionary containing the average result per-user across different metrics.
    """

    def __init__(self, model_name, metric_avg_results, metric_user_results):
        self.model_name = model_name
        self.metric_avg_results = metric_avg_results
        self.metric_user_results = metric_user_results

    def __str__(self):
        headers = list(self.metric_avg_results.keys())
        data = [[NUM_FMT.format(v) for v in self.metric_avg_results.values()]]
        return _table_format(data, headers, index=[self.model_name], h_bars=[1])

# movielens
# Copyright 2018 The Cornac Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""Link to the data: https://grouplens.org/datasets/movielens/"""

from typing import List
from collections import namedtuple



VALID_DATA_FORMATS = ["UIR", "UIRT"]

MovieLens = namedtuple("MovieLens", ["url", "unzip", "path", "sep", "skip"])
ML_DATASETS = {
    "100K": MovieLens(
        "http://files.grouplens.org/datasets/movielens/ml-100k/u.data",
        False,
        "ml-100k/u.data",
        "\t",
        0,
    ),
    "1M": MovieLens(
        "http://files.grouplens.org/datasets/movielens/ml-1m.zip",
        True,
        "ml-1m/ratings.dat",
        "::",
        0,
    ),
    "10M": MovieLens(
        "http://files.grouplens.org/datasets/movielens/ml-10m.zip",
        True,
        "ml-10M100K/ratings.dat",
        "::",
        0,
    ),
    "20M": MovieLens(
        "http://files.grouplens.org/datasets/movielens/ml-20m.zip",
        True,
        "ml-20m/ratings.csv",
        ",",
        1,
    ),
}


def load_feedback(fmt="UIR", variant="100K", reader=None):
    """Load the user-item ratings of one of the MovieLens datasets

    Parameters
    ----------
    fmt: str, default: 'UIR'
        Data format to be returned, one of ['UIR', 'UIRT'].

    variant: str, optional, default: '100K'
        Specifies which MovieLens dataset to load, one of ['100K', '1M', '10M', '20M'].

    reader: `obj:cornac.data.Reader`, optional, default: None
        Reader object used to read the data.

    Returns
    -------
    data: array-like
        Data in the form of a list of tuples depending on the given data format.
    """

    fmt = validate_format(fmt, VALID_DATA_FORMATS)

    ml = ML_DATASETS.get(variant.upper(), None)
    if ml is None:
        raise ValueError("variant must be one of {}.".format(ML_DATASETS.keys()))

    fpath = cache(url=ml.url, unzip=ml.unzip, relative_path=ml.path)
    print('fpath ml_1m_ratings: ', fpath)
    reader = Reader() if reader is None else reader
    return reader.read(fpath, fmt, sep=ml.sep, skip_lines=ml.skip)


def load_plot():
    """Load the plots of movies provided @ http://dm.postech.ac.kr/~cartopy/ConvMF/

    Returns
    -------
    texts: List
        List of text documents, one per item.

    ids: List
        List of item ids aligned with indices in `texts`.
    """
    fpath = cache(
        url="https://static.preferred.ai/cornac/datasets/movielens/ml_plot.zip",
        unzip=True,
        relative_path="movielens/ml_plot.dat",
    )
    print('fpath ml_plots: ', fpath)
    texts, ids = read_text(fpath, sep="::")
    return texts, ids

# base method
def rating_eval(model, metrics, test_set, user_based=False, verbose=False):
    """Evaluate model on provided rating metrics.

    Parameters
    ----------
    model: :obj:`cornac.models.Recommender`, required
        Recommender model to be evaluated.

    metrics: :obj:`iterable`, required
        List of rating metrics :obj:`cornac.metrics.RatingMetric`.

    test_set: :obj:`cornac.data.Dataset`, required
        Dataset to be used for evaluation.

    user_based: bool, optional, default: False
        Evaluation mode. Whether results are averaging based on number of users or number of ratings.

    verbose: bool, optional, default: False
        Output evaluation progress.

    Returns
    -------
    res: (List, List)
        Tuple of two lists:
         - average result for each of the metrics
         - average result per user for each of the metrics

    """

    if len(metrics) == 0:
        return [], []

    avg_results = []
    user_results = []

    (u_indices, i_indices, r_values) = test_set.uir_tuple
    r_preds = np.fromiter(
        tqdm(
            (
                model.rate(user_idx, item_idx).item()
                for user_idx, item_idx in zip(u_indices, i_indices)
            ),
            desc="Rating",
            disable=not verbose,
            miniters=100,
            total=len(u_indices),
        ),
        dtype="float",
    )

    gt_mat = test_set.csr_matrix
    pd_mat = csr_matrix((r_preds, (u_indices, i_indices)), shape=gt_mat.shape)

    test_user_indices = set(u_indices)
    for mt in metrics:
        if user_based:  # averaging over users
            user_results.append(
                {
                    user_idx: mt.compute(
                        gt_ratings=gt_mat.getrow(user_idx).data,
                        pd_ratings=pd_mat.getrow(user_idx).data,
                    ).item()
                    for user_idx in test_user_indices
                }
            )
            avg_results.append(sum(user_results[-1].values()) / len(user_results[-1]))
        else:  # averaging over ratings
            user_results.append({})
            avg_results.append(mt.compute(gt_ratings=r_values, pd_ratings=r_preds))

    return avg_results, user_results


def ranking_eval(
    model,
    metrics,
    train_set,
    test_set,
    val_set=None,
    rating_threshold=1.0,
    exclude_unknowns=True,
    verbose=False,
):
    """Evaluate model on provided ranking metrics.

    Parameters
    ----------
    model: :obj:`cornac.models.Recommender`, required
        Recommender model to be evaluated.

    metrics: :obj:`iterable`, required
        List of rating metrics :obj:`cornac.metrics.RankingMetric`.

    train_set: :obj:`cornac.data.Dataset`, required
        Dataset to be used for model training. This will be used to exclude
        observations already appeared during training.

    test_set: :obj:`cornac.data.Dataset`, required
        Dataset to be used for evaluation.

    val_set: :obj:`cornac.data.Dataset`, optional, default: None
        Dataset to be used for model selection. This will be used to exclude
        observations already appeared during validation.

    rating_threshold: float, optional, default: 1.0
        The threshold to convert ratings into positive or negative feedback.

    exclude_unknowns: bool, optional, default: True
        Ignore unknown users and items during evaluation.

    verbose: bool, optional, default: False
        Output evaluation progress.

    Returns
    -------
    res: (List, List)
        Tuple of two lists:
         - average result for each of the metrics
         - average result per user for each of the metrics

    """

    if len(metrics) == 0:
        return [], []

    max_k = max(m.k for m in metrics)

    avg_results = []
    user_results = [{} for _ in enumerate(metrics)]

    test_mat = test_set.csr_matrix
    train_mat = train_set.csr_matrix
    val_mat = None if val_set is None else val_set.csr_matrix

    def pos_items(csr_row):
        return [
            item_idx
            for (item_idx, rating) in zip(csr_row.indices, csr_row.data)
            if rating >= rating_threshold
        ]

    test_user_indices = set(test_set.uir_tuple[0])
    for user_idx in tqdm(
        test_user_indices, desc="Ranking", disable=not verbose, miniters=100
    ):
        test_pos_items = pos_items(test_mat.getrow(user_idx))
        if len(test_pos_items) == 0:
            continue

        # binary mask for ground-truth positive items
        u_gt_pos_mask = np.zeros(test_set.num_items, dtype="int")
        u_gt_pos_mask[test_pos_items] = 1

        val_pos_items = [] if val_mat is None else pos_items(val_mat.getrow(user_idx))
        train_pos_items = (
            pos_items(train_mat.getrow(user_idx))
            if user_idx < train_mat.shape[0]
            else []
        )

        # binary mask for ground-truth negative items, removing all positive items
        u_gt_neg_mask = np.ones(test_set.num_items, dtype="int")
        u_gt_neg_mask[test_pos_items + val_pos_items + train_pos_items] = 0

        # filter items being considered for evaluation
        if exclude_unknowns:
            u_gt_pos_mask = u_gt_pos_mask[: train_set.num_items]
            u_gt_neg_mask = u_gt_neg_mask[: train_set.num_items]

        item_indices = np.nonzero(u_gt_pos_mask + u_gt_neg_mask)[0]
        u_gt_pos_items = np.nonzero(u_gt_pos_mask)[0]
        u_gt_neg_items = np.nonzero(u_gt_neg_mask)[0]

        item_rank, item_scores = model.rank(
            user_idx=user_idx, item_indices=item_indices, k=max_k
        )

        for i, mt in enumerate(metrics):
            mt_score = mt.compute(
                gt_pos=u_gt_pos_items,
                gt_neg=u_gt_neg_items,
                pd_rank=item_rank,
                pd_scores=item_scores,
                item_indices=item_indices,
            )
            user_results[i][user_idx] = mt_score

    # avg results of ranking metrics
    for i, mt in enumerate(metrics):
        avg_results.append(sum(user_results[i].values()) / len(user_results[i]))

    return avg_results, user_results


class BaseMethod:
    """Base Evaluation Method

    Parameters
    ----------
    data: array-like, required
        Raw preference data in the triplet format [(user_id, item_id, rating_value)].

    fmt: str, default: 'UIR'
        Format of the input data. Currently, we are supporting:

        'UIR': User, Item, Rating
        'UIRT': User, Item, Rating, Timestamp

    rating_threshold: float, optional, default: 1.0
        Threshold used to binarize rating values into positive or negative feedback for
        model evaluation using ranking metrics (rating metrics are not affected).

    seed: int, optional, default: None
        Random seed for reproducibility.

    exclude_unknowns: bool, optional, default: True
        If `True`, unknown users and items will be ignored during model evaluation.

    verbose: bool, optional, default: False
        Output running log.

    """

    def __init__(
        self,
        data=None,
        fmt="UIR",
        rating_threshold=1.0,
        seed=None,
        exclude_unknowns=True,
        verbose=False,
        **kwargs
    ):
        self.data = data
        self.fmt = fmt
        self.train_set = None
        self.test_set = None
        self.val_set = None
        self.rating_threshold = rating_threshold
        self.exclude_unknowns = exclude_unknowns
        self.verbose = verbose
        self.seed = seed
        self.rng = get_rng(seed)
        self.global_uid_map = kwargs.get("global_uid_map", OrderedDict())
        self.global_iid_map = kwargs.get("global_iid_map", OrderedDict())

        self.user_feature = kwargs.get("user_feature", None)
        self.user_text = kwargs.get("user_text", None)
        self.user_image = kwargs.get("user_image", None)
        self.user_graph = kwargs.get("user_graph", None)
        self.item_feature = kwargs.get("item_feature", None)
        self.item_text = kwargs.get("item_text", None)
        self.item_image = kwargs.get("item_image", None)
        self.item_graph = kwargs.get("item_graph", None)
        self.sentiment = kwargs.get("sentiment", None)
        self.review_text = kwargs.get("review_text", None)

        if verbose:
            print("rating_threshold = {:.1f}".format(rating_threshold))
            print("exclude_unknowns = {}".format(exclude_unknowns))

    @property
    def total_users(self):
        return len(self.global_uid_map)

    @property
    def total_items(self):
        return len(self.global_iid_map)

    @property
    def user_feature(self):
        return self.__user_feature

    @property
    def user_text(self):
        return self.__user_text

    @user_feature.setter
    def user_feature(self, input_modality):
        if input_modality is not None and not isinstance(
            input_modality, FeatureModality
        ):
            raise ValueError(
                "input_modality has to be instance of FeatureModality but {}".format(
                    type(input_modality)
                )
            )
        self.__user_feature = input_modality

    @user_text.setter
    def user_text(self, input_modality):
        if input_modality is not None and not isinstance(input_modality, TextModality):
            raise ValueError(
                "input_modality has to be instance of TextModality but {}".format(
                    type(input_modality)
                )
            )
        self.__user_text = input_modality

    @property
    def user_image(self):
        return self.__user_image

    @user_image.setter
    def user_image(self, input_modality):
        if input_modality is not None and not isinstance(input_modality, ImageModality):
            raise ValueError(
                "input_modality has to be instance of ImageModality but {}".format(
                    type(input_modality)
                )
            )
        self.__user_image = input_modality

    @property
    def user_graph(self):
        return self.__user_graph

    @user_graph.setter
    def user_graph(self, input_modality):
        if input_modality is not None and not isinstance(input_modality, GraphModality):
            raise ValueError(
                "input_modality has to be instance of GraphModality but {}".format(
                    type(input_modality)
                )
            )
        self.__user_graph = input_modality

    @property
    def item_feature(self):
        return self.__item_feature

    @property
    def item_text(self):
        return self.__item_text

    @item_feature.setter
    def item_feature(self, input_modality):
        if input_modality is not None and not isinstance(
            input_modality, FeatureModality
        ):
            raise ValueError(
                "input_modality has to be instance of FeatureModality but {}".format(
                    type(input_modality)
                )
            )
        self.__item_feature = input_modality

    @item_text.setter
    def item_text(self, input_modality):
        if input_modality is not None and not isinstance(input_modality, TextModality):
            raise ValueError(
                "input_modality has to be instance of TextModality but {}".format(
                    type(input_modality)
                )
            )
        self.__item_text = input_modality

    @property
    def item_image(self):
        return self.__item_image

    @item_image.setter
    def item_image(self, input_modality):
        if input_modality is not None and not isinstance(input_modality, ImageModality):
            raise ValueError(
                "input_modality has to be instance of ImageModality but {}".format(
                    type(input_modality)
                )
            )
        self.__item_image = input_modality

    @property
    def item_graph(self):
        return self.__item_graph

    @item_graph.setter
    def item_graph(self, input_modality):
        if input_modality is not None and not isinstance(input_modality, GraphModality):
            raise ValueError(
                "input_modality has to be instance of GraphModality but {}".format(
                    type(input_modality)
                )
            )
        self.__item_graph = input_modality

    @property
    def sentiment(self):
        return self.__sentiment

    @sentiment.setter
    def sentiment(self, input_modality):
        if input_modality is not None and not isinstance(
            input_modality, SentimentModality
        ):
            raise ValueError(
                "input_modality has to be instance of SentimentModality but {}".format(
                    type(input_modality)
                )
            )
        self.__sentiment = input_modality

    @property
    def review_text(self):
        return self.__review_text

    @review_text.setter
    def review_text(self, input_modality):
        if input_modality is not None and not isinstance(
            input_modality, ReviewModality
        ):
            raise ValueError(
                "input_modality has to be instance of ReviewModality but {}".format(
                    type(input_modality)
                )
            )
        self.__review_text = input_modality

    def _reset(self):
        """Reset the random number generator for reproducibility"""
        self.rng = get_rng(self.seed)
        self.test_set = self.test_set.reset()

    @staticmethod
    def organize_metrics(metrics):
        """Organize metrics according to their types (rating or raking)

        Parameters
        ----------
        metrics: :obj:`iterable`
            List of metrics.

        """
        if isinstance(metrics, dict):
            rating_metrics = metrics.get("rating", [])
            ranking_metrics = metrics.get("ranking", [])
        elif isinstance(metrics, list):
            rating_metrics = []
            ranking_metrics = []
            for mt in metrics:
                if isinstance(mt, RatingMetric):
                    rating_metrics.append(mt)
                elif isinstance(mt, RankingMetric) and hasattr(mt.k, "__len__"):
                    ranking_metrics.extend(
                        [mt.__class__(k=_k) for _k in sorted(set(mt.k))]
                    )
                else:
                    ranking_metrics.append(mt)
        else:
            raise ValueError("Type of metrics has to be either dict or list!")

        # sort metrics by name
        rating_metrics = sorted(rating_metrics, key=lambda mt: mt.name)
        ranking_metrics = sorted(ranking_metrics, key=lambda mt: mt.name)
        return rating_metrics, ranking_metrics

    def _build_datasets(self, train_data, test_data, val_data=None):
        self.train_set = Dataset.build(
            data=train_data,
            fmt=self.fmt,
            global_uid_map=self.global_uid_map,
            global_iid_map=self.global_iid_map,
            seed=self.seed,
            exclude_unknowns=False,
        )
        if self.verbose:
            print("---")
            print("Training data:")
            print("Number of users = {}".format(self.train_set.num_users))
            print("Number of items = {}".format(self.train_set.num_items))
            print("Number of ratings = {}".format(self.train_set.num_ratings))
            print("Max rating = {:.1f}".format(self.train_set.max_rating))
            print("Min rating = {:.1f}".format(self.train_set.min_rating))
            print("Global mean = {:.1f}".format(self.train_set.global_mean))
            logging.info('Starting training model ConvMF')

        self.test_set = Dataset.build(
            data=test_data,
            fmt=self.fmt,
            global_uid_map=self.global_uid_map,
            global_iid_map=self.global_iid_map,
            seed=self.seed,
            exclude_unknowns=self.exclude_unknowns,
        )
        if self.verbose:
            print("---")
            print("Test data:")
            print("Number of users = {}".format(len(self.test_set.uid_map)))
            print("Number of items = {}".format(len(self.test_set.iid_map)))
            print("Number of ratings = {}".format(self.test_set.num_ratings))
            print(
                "Number of unknown users = {}".format(
                    self.test_set.num_users - self.train_set.num_users
                )
            )
            print(
                "Number of unknown items = {}".format(
                    self.test_set.num_items - self.train_set.num_items
                )
            )

        if val_data is not None and len(val_data) > 0:
            self.val_set = Dataset.build(
                data=val_data,
                fmt=self.fmt,
                global_uid_map=self.global_uid_map,
                global_iid_map=self.global_iid_map,
                seed=self.seed,
                exclude_unknowns=self.exclude_unknowns,
            )
            if self.verbose:
                print("---")
                print("Validation data:")
                print("Number of users = {}".format(len(self.val_set.uid_map)))
                print("Number of items = {}".format(len(self.val_set.iid_map)))
                print("Number of ratings = {}".format(self.val_set.num_ratings))

        if self.verbose:
            print("---")
            print("Total users = {}".format(self.total_users))
            print("Total items = {}".format(self.total_items))

    def _build_modalities(self):
        for user_modality in [
            self.user_feature,
            self.user_text,
            self.user_image,
            self.user_graph,
        ]:
            if user_modality is None:
                continue
            user_modality.build(
                id_map=self.global_uid_map,
                uid_map=self.train_set.uid_map,
                iid_map=self.train_set.iid_map,
                dok_matrix=self.train_set.dok_matrix,
            )

        for item_modality in [
            self.item_feature,
            self.item_text,
            self.item_image,
            self.item_graph,
        ]:
            if item_modality is None:
                continue
            item_modality.build(
                id_map=self.global_iid_map,
                uid_map=self.train_set.uid_map,
                iid_map=self.train_set.iid_map,
                dok_matrix=self.train_set.dok_matrix,
            )

        for modality in [self.sentiment, self.review_text]:
            if modality is None:
                continue
            modality.build(
                uid_map=self.train_set.uid_map,
                iid_map=self.train_set.iid_map,
                dok_matrix=self.train_set.dok_matrix,
            )

        self.add_modalities(
            user_feature=self.user_feature,
            user_text=self.user_text,
            user_image=self.user_image,
            user_graph=self.user_graph,
            item_feature=self.item_feature,
            item_text=self.item_text,
            item_image=self.item_image,
            item_graph=self.item_graph,
            sentiment=self.sentiment,
            review_text=self.review_text,
        )

    def add_modalities(self, **kwargs):
        """
        Add successfully built modalities to all datasets. This is handy for
        seperately built modalities that are not invoked in the build method.
        """
        self.user_feature = kwargs.get("user_feature", None)
        self.user_text = kwargs.get("user_text", None)
        self.user_image = kwargs.get("user_image", None)
        self.user_graph = kwargs.get("user_graph", None)
        self.item_feature = kwargs.get("item_feature", None)
        self.item_text = kwargs.get("item_text", None)
        self.item_image = kwargs.get("item_image", None)
        self.item_graph = kwargs.get("item_graph", None)
        self.sentiment = kwargs.get("sentiment", None)
        self.review_text = kwargs.get("review_text", None)

        for data_set in [self.train_set, self.test_set, self.val_set]:
            if data_set is None:
                continue
            data_set.add_modalities(
                user_feature=self.user_feature,
                user_text=self.user_text,
                user_image=self.user_image,
                user_graph=self.user_graph,
                item_feature=self.item_feature,
                item_text=self.item_text,
                item_image=self.item_image,
                item_graph=self.item_graph,
                sentiment=self.sentiment,
                review_text=self.review_text,
            )

    def build(self, train_data, test_data, val_data=None):
        if train_data is None or len(train_data) == 0:
            raise ValueError("train_data is required but None or empty!")
        if test_data is None or len(test_data) == 0:
            raise ValueError("test_data is required but None or empty!")

        self.global_uid_map.clear()
        self.global_iid_map.clear()

        self._build_datasets(train_data, test_data, val_data)
        self._build_modalities()

        return self

    @staticmethod
    def eval(
        model,
        train_set,
        test_set,
        val_set,
        rating_threshold,
        exclude_unknowns,
        user_based,
        rating_metrics,
        ranking_metrics,
        verbose,
    ):
        """Running evaluation for rating and ranking metrics respectively."""
        metric_avg_results = OrderedDict()
        metric_user_results = OrderedDict()

        avg_results, user_results = rating_eval(
            model=model,
            metrics=rating_metrics,
            test_set=test_set,
            user_based=user_based,
            verbose=verbose,
        )
        for i, mt in enumerate(rating_metrics):
            metric_avg_results[mt.name] = avg_results[i]
            metric_user_results[mt.name] = user_results[i]

        avg_results, user_results = ranking_eval(
            model=model,
            metrics=ranking_metrics,
            train_set=train_set,
            test_set=test_set,
            val_set=val_set,
            rating_threshold=rating_threshold,
            exclude_unknowns=exclude_unknowns,
            verbose=verbose,
        )
        for i, mt in enumerate(ranking_metrics):
            metric_avg_results[mt.name] = avg_results[i]
            metric_user_results[mt.name] = user_results[i]

        return Result(model.name, metric_avg_results, metric_user_results)

    def evaluate(self, model, metrics, user_based, show_validation=True):
        """Evaluate given models according to given metrics. Supposed to be called by Experiment.

        Parameters
        ----------
        model: :obj:`cornac.models.Recommender`
            Recommender model to be evaluated.

        metrics: :obj:`iterable`
            List of metrics.

        user_based: bool, required
            Evaluation strategy for the rating metrics. Whether results
            are averaging based on number of users or number of ratings.

        show_validation: bool, optional, default: True
            Whether to show the results on validation set (if exists).

        Returns
        -------
        res: :obj:`cornac.experiment.Result`
        """
        if self.train_set is None:
            raise ValueError("train_set is required but None!")
        if self.test_set is None:
            raise ValueError("test_set is required but None!")

        self._reset()

        ###########
        # FITTING #
        ###########
        if self.verbose:
            print("\n[{}] Training started!".format(model.name))

        start = time.time()
        model.fit(self.train_set, self.val_set)
        train_time = time.time() - start

        ##############
        # EVALUATION #
        ##############
        if self.verbose:
            print("\n[{}] Evaluation started!".format(model.name))

        rating_metrics, ranking_metrics = self.organize_metrics(metrics)

        start = time.time()
        model.transform(self.test_set)
        test_result = self.eval(
            model=model,
            train_set=self.train_set,
            test_set=self.test_set,
            val_set=self.val_set,
            rating_threshold=self.rating_threshold,
            exclude_unknowns=self.exclude_unknowns,
            rating_metrics=rating_metrics,
            ranking_metrics=ranking_metrics,
            user_based=user_based,
            verbose=self.verbose,
        )
        test_time = time.time() - start
        test_result.metric_avg_results["Train (s)"] = train_time
        test_result.metric_avg_results["Test (s)"] = test_time

        val_result = None
        if show_validation and self.val_set is not None:
            start = time.time()
            model.transform(self.val_set)
            val_result = self.eval(
                model=model,
                train_set=self.train_set,
                test_set=self.val_set,
                val_set=None,
                rating_threshold=self.rating_threshold,
                exclude_unknowns=self.exclude_unknowns,
                rating_metrics=rating_metrics,
                ranking_metrics=ranking_metrics,
                user_based=user_based,
                verbose=self.verbose,
            )
            val_time = time.time() - start
            val_result.metric_avg_results["Time (s)"] = val_time
        print('test_result: ', test_result)
        try:
          print('val_result: ', val_result)
          model.transform(self.train_set)
          print('after transform')
          train_result = self.eval(
                model=model,
                train_set=self.train_set,
                test_set=self.train_set,
                val_set=None,
                rating_threshold=self.rating_threshold,
                exclude_unknowns=self.exclude_unknowns,
                rating_metrics=rating_metrics,
                ranking_metrics=ranking_metrics,
                user_based=user_based,
                verbose=self.verbose,
            )
          print('train_result: ',train_result)
        except Exception as e:
          print('error when cal in train set: ', e)
        return test_result, val_result

    @classmethod
    def from_splits(
        cls,
        train_data,
        test_data,
        val_data=None,
        fmt="UIR",
        rating_threshold=1.0,
        exclude_unknowns=False,
        seed=None,
        verbose=False,
        **kwargs
    ):
        """Constructing evaluation method given data.

        Parameters
        ----------
        train_data: array-like
            Training data

        test_data: array-like
            Test data

        val_data: array-like, optional, default: None
            Validation data

        fmt: str, default: 'UIR'
            Format of the input data. Currently, we are supporting:

            'UIR': User, Item, Rating
            'UIRT': User, Item, Rating, Timestamp

        rating_threshold: float, default: 1.0
            Threshold to decide positive or negative preferences.

        exclude_unknowns: bool, default: False
            Whether to exclude unknown users/items in evaluation.

        seed: int, optional, default: None
            Random seed for reproduce the splitting.

        verbose: bool, default: False
            The verbosity flag.

        Returns
        -------
        method: :obj:`<cornac.eval_methods.BaseMethod>`
            Evaluation method object.

        """
        method = cls(
            fmt=fmt,
            rating_threshold=rating_threshold,
            exclude_unknowns=exclude_unknowns,
            seed=seed,
            verbose=verbose,
            **kwargs
        )

        return method.build(
            train_data=train_data, test_data=test_data, val_data=val_data
        )

# ratio split
from math import ceil

class RatioSplit(BaseMethod):
    """Splitting data into training, validation, and test sets based on provided sizes.
    Data is always shuffled before split.

    Parameters
    ----------
    data: array-like, required
        Raw preference data in the triplet format [(user_id, item_id, rating_value)].

    test_size: float, optional, default: 0.2
        The proportion of the test set, \
        if > 1 then it is treated as the size of the test set.

    val_size: float, optional, default: 0.0
        The proportion of the validation set, \
        if > 1 then it is treated as the size of the validation set.

    rating_threshold: float, optional, default: 1.0
        Threshold used to binarize rating values into positive or negative feedback for
        model evaluation using ranking metrics (rating metrics are not affected).

    seed: int, optional, default: None
        Random seed for reproducibility.

    exclude_unknowns: bool, optional, default: True
        If `True`, unknown users and items will be ignored during model evaluation.

    verbose: bool, optional, default: False
        Output running log.

    """

    def __init__(
        self,
        data,
        test_size=0.2,
        val_size=0.0,
        rating_threshold=1.0,
        seed=None,
        exclude_unknowns=True,
        verbose=False,
        **kwargs,
    ):
        super().__init__(
            data=data,
            rating_threshold=rating_threshold,
            seed=seed,
            exclude_unknowns=exclude_unknowns,
            verbose=verbose,
            **kwargs,
        )

        self.train_size, self.val_size, self.test_size = self.validate_size(
            val_size=val_size,
            test_size=test_size,
            data_size=kwargs.get("data_size", len(data)),
        )
        self._split()

    @staticmethod
    def validate_size(val_size, test_size, data_size):
        if val_size is None:
            val_size = 0.0
        elif val_size < 0:
            raise ValueError("val_size={} should be greater than zero".format(val_size))
        elif val_size >= data_size:
            raise ValueError(
                f"val_size={val_size} should be smaller than data_size={data_size}"
            )

        if test_size is None:
            test_size = 0.0
        elif test_size < 0:
            raise ValueError(f"test_size={test_size} should be greater than zero")
        elif test_size >= data_size:
            raise ValueError(
                f"test_size={test_size} should be smaller than data_size={data_size}"
            )

        if val_size < 1:
            val_size = ceil(val_size * data_size)
        if test_size < 1:
            test_size = ceil(test_size * data_size)

        val_test_size = val_size + test_size
        if val_test_size >= data_size:
            raise ValueError(
                f"val_size + test_size ({val_test_size}) should be smaller than data_size={data_size}"
            )

        train_size = data_size - (val_size + test_size)

        return int(train_size), int(val_size), int(test_size)

    def _split(self):
        data_idx = self.rng.permutation(len(self.data))
        train_idx = data_idx[: self.train_size]
        test_idx = data_idx[-self.test_size :]
        val_idx = data_idx[self.train_size : -self.test_size]

        train_data = safe_indexing(self.data, train_idx)
        test_data = safe_indexing(self.data, test_idx)
        val_data = safe_indexing(self.data, val_idx) if len(val_idx) > 0 else None

        self.build(train_data=train_data, test_data=test_data, val_data=val_data)

# xavier_uniform
def uniform(shape=None, low=0.0, high=1.0, random_state=None, dtype=np.float32):
    """
    Draw samples from a uniform distribution.

    Parameters
    ----------
    shape : int or tuple of ints, optional
        Output shape. If shape is ``None`` (default), a single value is returned.
    low : float or array_like of floats, optional
        Lower boundary of the output interval.  All values generated will be
        greater than or equal to low.  The default value is 0.
    high : float or array_like of floats
        Upper boundary of the output interval.  All values generated will be
        less than high.  The default value is 1.0.
    random_state : int or np.random.RandomState, optional
        If an integer is given, it will be used as seed value for creating a RandomState.
    dtype : str or dtype
        Returned data-type for the output array.

    Returns
    -------
    out : ndarray or scalar
        Drawn samples from the parameterized uniform distribution.
    """
    return get_rng(random_state).uniform(low, high, shape).astype(dtype)

def xavier_uniform(shape, random_state=None, dtype=np.float32):
    """Return a numpy array by performing 'Xavier' initializer
    also known as 'Glorot' initializer on Uniform distribution.

    Parameters
    ----------
    shape : int or tuple of ints
        Output shape.
    random_state : int or np.random.RandomState, optional
        If an integer is given, it will be used as seed value for creating a RandomState.
    dtype : str or dtype
        Returned data-type for the output array.

    Returns
    -------
    out : ndarray
        Output matrix.

    References
    ----------
    ** Xavier Glorot and Yoshua Bengio (2010):
    [Understanding the difficulty of training deep feedforward neural networks.
    International conference on artificial intelligence and statistics.]
    (http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
    """
    assert len(shape) == 2  # only support matrix
    std = np.sqrt(2.0 / np.sum(shape))
    limit = np.sqrt(3.0) * std
    return uniform(shape, -limit, limit, random_state, dtype)

class CornacException(Exception):
    """Exception base class to extend from"""

    pass


class ScoreException(CornacException):
    """Exception raised in score function when facing unknowns"""

    pass

# Commented out IPython magic to ensure Python compatibility.
# convmf
import copy
import inspect
import os
import pickle
import warnings
from datetime import datetime
from glob import glob
import torch
MEASURE_DOT = "dot product aka. inner product"
class ANNMixin:
    """Mixin class for Approximate Nearest Neighbor Search."""

    _ann_supported = True

    def get_vector_measure(self):
        """Getting a valid choice of vector measurement in ANNMixin._measures.

        Returns
        -------
        :raise NotImplementedError
        """
        raise NotImplementedError()

    def get_user_vectors(self):
        """Getting a matrix of user vectors serving as query for ANN search.

        Returns
        -------
        :raise NotImplementedError
        """
        raise NotImplementedError()

    def get_item_vectors(self):
        """Getting a matrix of item vectors used for building the index for ANN search.

        Returns
        -------
        :raise NotImplementedError
        """
        raise NotImplementedError()


class Recommender:
    """Generic class for a recommender model. All recommendation models should inherit from this class.

    Parameters
    ----------------
    name: str, required
        Name of the recommender model.

    trainable: boolean, optional, default: True
        When False, the model is not trainable.

    verbose: boolean, optional, default: False
        When True, running logs are displayed.

    Attributes
    ----------
    num_users: int
        Number of users in training data.

    num_items: int
        Number of items in training data.

    total_users: int
        Number of users in training, validation, and test data.
        In other words, this includes unknown/unseen users.

    total_items: int
        Number of items in training, validation, and test data.
        In other words, this includes unknown/unseen items.

    uid_map: int
        Global mapping of user ID-index.

    iid_map: int
        Global mapping of item ID-index.

    max_rating: float
        Maximum value among the rating observations.

    min_rating: float
        Minimum value among the rating observations.

    global_mean: float
        Average value over the rating observations.
    """

    def __init__(self, name, trainable=True, verbose=False):
        self.name = name
        self.trainable = trainable
        self.verbose = verbose
        self.is_fitted = False

        # attributes to be ignored when saving model
        self.ignored_attrs = ["train_set", "val_set", "test_set"]

        # useful information getting from train_set for prediction
        self.num_users = None
        self.num_items = None
        self.uid_map = None
        self.iid_map = None
        self.max_rating = None
        self.min_rating = None
        self.global_mean = None

        self.__user_ids = None
        self.__item_ids = None

    @property
    def total_users(self):
        """Total number of users including users in test and validation if exists"""
        return len(self.uid_map) if self.uid_map is not None else self.num_users

    @property
    def total_items(self):
        """Total number of items including users in test and validation if exists"""
        return len(self.iid_map) if self.iid_map is not None else self.num_items

    @property
    def user_ids(self):
        """Return the list of raw user IDs"""
        if self.__user_ids is None:
            self.__user_ids = list(self.uid_map.keys())
        return self.__user_ids

    @property
    def item_ids(self):
        """Return the list of raw item IDs"""
        if self.__item_ids is None:
            self.__item_ids = list(self.iid_map.keys())
        return self.__item_ids

    def reset_info(self):
        self.best_value = -np.Inf
        self.best_epoch = 0
        self.current_epoch = 0
        self.stopped_epoch = 0
        self.wait = 0

    def __deepcopy__(self, memo):
        cls = self.__class__
        result = cls.__new__(cls)
        ignored_attrs = set(self.ignored_attrs)
        for k, v in self.__dict__.items():
            if k in ignored_attrs:
                continue
            setattr(result, k, copy.deepcopy(v))
        return result

    @classmethod
    def _get_init_params(cls):
        """Get initial parameters from the model constructor"""
        init = getattr(cls.__init__, "deprecated_original", cls.__init__)
        if init is object.__init__:
            return []

        init_signature = inspect.signature(init)
        parameters = [p for p in init_signature.parameters.values() if p.name != "self"]

        return sorted([p.name for p in parameters])

    def clone(self, new_params=None):
        """Clone an instance of the model object.

        Parameters
        ----------
        new_params: dict, optional, default: None
            New parameters for the cloned instance.

        Returns
        -------
        object: :obj:`cornac.models.Recommender`
        """
        new_params = {} if new_params is None else new_params
        init_params = {}
        for name in self._get_init_params():
            init_params[name] = new_params.get(name, copy.deepcopy(getattr(self, name)))

        return self.__class__(**init_params)

    def save(self, save_dir=None, save_trainset=False):
        """Save a recommender model to the filesystem.

        Parameters
        ----------
        save_dir: str, default: None
            Path to a directory for the model to be stored.

        save_trainset: bool, default: False
            Save train_set together with the model. This is useful
            if we want to deploy model later because train_set is
            required for certain evaluation steps.

        Returns
        -------
        model_file : str
            Path to the model file stored on the filesystem.
        """
        if save_dir is None:
            return

        model_dir = os.path.join(save_dir, self.name)
        os.makedirs(model_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S-%f")
#         model_file = os.path.join(model_dir, "{}.pkl".format(timestamp))
        model_file = "./model_convmf/weights_" + "{}.pkl".format(timestamp)
        print('model_file dir: ', model_file)

        saved_model = copy.deepcopy(self)
        pickle.dump(saved_model, open(model_file, "wb"), protocol=pickle.HIGHEST_PROTOCOL)
        if self.verbose:
            print("{} model is saved to {}".format(self.name, model_file))

        if save_trainset:
            pickle.dump(
                self.train_set,
                open(model_file + ".trainset", "wb"),
                protocol=pickle.HIGHEST_PROTOCOL,
            )

        return model_file

    @staticmethod
    def load(model_path, trainable=False):
        """Load a recommender model from the filesystem.

        Parameters
        ----------
        model_path: str, required
            Path to a file or directory where the model is stored. If a directory is
            provided, the latest model will be loaded.

        trainable: boolean, optional, default: False
            Set it to True if you would like to finetune the model. By default,
            the model parameters are assumed to be fixed after being loaded.

        Returns
        -------
        self : object
        """
        if os.path.isdir(model_path):
            model_file = sorted(glob("{}/*.pkl".format(model_path)))[-1]
        else:
            model_file = model_path
        print('load model from file: ', model_file)
        model = pickle.load(open(model_file, "rb"))
        model.trainable = trainable
        model.load_from = model_file  # for further loading
        return model

#     def save(self, save_dir=None, save_trainset=False):
#         """Save a recommender model to the filesystem.

#         Parameters
#         ----------
#         save_dir: str, default: None
#             Path to a directory for the model to be stored.

#         save_trainset: bool, default: False
#             Save train_set together with the model. This is useful
#             if we want to deploy model later because train_set is
#             required for certain evaluation steps.

#         Returns
#         -------
#         model_file : str
#             Path to the model file stored on the filesystem.
#         """
#         if save_dir is None:
#             return

#         model_dir = os.path.join(save_dir, self.name)
#         os.makedirs(model_dir, exist_ok=True)
#         timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S-%f")
#         model_file = os.path.join(model_dir, f"weights_{timestamp}.pt")
#         print('model_file dir: ', model_file)

#         saved_model = copy.deepcopy(self)
#         torch.save(saved_model, model_file)
#         if self.verbose:
#             print(f"{self.name} model is saved to {model_file}")

#         if save_trainset:
#             trainset_file = model_file + ".trainset"
#             torch.save(self.train_set, trainset_file)

#         return model_file

#     @staticmethod
#     def load(model_path, trainable=False):
#         """Load a recommender model from the filesystem.

#         Parameters
#         ----------
#         model_path: str, required
#             Path to a file or directory where the model is stored. If a directory is
#             provided, the latest model will be loaded.

#         trainable: boolean, optional, default: False
#             Set it to True if you would like to finetune the model. By default,
#             the model parameters are assumed to be fixed after being loaded.

#         Returns
#         -------
#         self : object
#         """
#         if os.path.isdir(model_path):
#             model_file = sorted(glob(os.path.join(model_path, "*.pt")))[-1]
#         else:
#             model_file = model_path

#         model = torch.load(model_file)
#         model.trainable = trainable
#         model.load_from = model_file  # for further loading
#         return model

    def fit(self, train_set, val_set=None):
        """Fit the model to observations.

        Parameters
        ----------
        train_set: :obj:`cornac.data.Dataset`, required
            User-Item preference data as well as additional modalities.

        val_set: :obj:`cornac.data.Dataset`, optional, default: None
            User-Item preference data for model selection purposes (e.g., early stopping).

        Returns
        -------
        self : object
        """
        if self.is_fitted:
            warnings.warn(
                "Model is already fitted. Re-fitting will overwrite the previous model."
            )

        self.reset_info()
        train_set.reset()
        if val_set is not None:
            val_set.reset()

        # get some useful information for prediction
        self.num_users = train_set.num_users
        self.num_items = train_set.num_items
        self.uid_map = train_set.uid_map
        self.iid_map = train_set.iid_map
        self.min_rating = train_set.min_rating
        self.max_rating = train_set.max_rating
        self.global_mean = train_set.global_mean

        # just for future wrapper to call fit(), not supposed to be used during prediction
        self.train_set = train_set
        self.val_set = val_set

        self.is_fitted = True

        return self

    def knows_user(self, user_idx):
        """Return whether the model knows user by its index

        Parameters
        ----------
        user_idx: int, required
            The index of the user (not the original user ID).

        Returns
        -------
        res : bool
            True if model knows the user from traning data, False otherwise.
        """
        return user_idx is not None and user_idx >= 0 and user_idx < self.num_users

    def knows_item(self, item_idx):
        """Return whether the model knows item by its index

        Parameters
        ----------
        item_idx: int, required
            The index of the item (not the original item ID).

        Returns
        -------
        res : bool
            True if model knows the item from traning data, False otherwise.
        """
        return item_idx is not None and item_idx >= 0 and item_idx < self.num_items

    def is_unknown_user(self, user_idx):
        """Return whether the model knows user by its index. Reverse of knows_user() function,
        for better readability in some cases.

        Parameters
        ----------
        user_idx: int, required
            The index of the user (not the original user ID).

        Returns
        -------
        res : bool
            True if model knows the user from traning data, False otherwise.
        """
        return not self.knows_user(user_idx)

    def is_unknown_item(self, item_idx):
        """Return whether the model knows item by its index. Reverse of knows_item() function,
        for better readability in some cases.

        Parameters
        ----------
        item_idx: int, required
            The index of the item (not the original item ID).

        Returns
        -------
        res : bool
            True if model knows the item from traning data, False otherwise.
        """
        return not self.knows_item(item_idx)

    def transform(self, test_set):
        """Transform test set into cached results accelerating the score function.
        This function is supposed to be called in the `cornac.eval_methods.BaseMethod`
        before evaluation step. It is optional for this function to be implemented.

        Parameters
        ----------
        test_set: :obj:`cornac.data.Dataset`, required
            User-Item preference data as well as additional modalities.

        """
        pass

    def score(self, user_idx, item_idx=None):
        """Predict the scores/ratings of a user for an item.

        Parameters
        ----------
        user_idx: int, required
            The index of the user for whom to perform score prediction.

        item_idx: int, optional, default: None
            The index of the item for which to perform score prediction.
            If None, scores for all known items will be returned.

        Returns
        -------
        res : A scalar or a Numpy array
            Relative scores that the user gives to the item or to all known items

        """
        # print('score in recommender class')
        raise NotImplementedError("The algorithm is not able to make score prediction!")

    def default_score(self):
        """Overwrite this function if your algorithm has special treatment for cold-start problem"""
        return self.global_mean

    def rate(self, user_idx, item_idx, clipping=True):
        """Give a rating score between pair of user and item

        Parameters
        ----------
        user_idx: int, required
            The index of the user for whom to perform item raking.

        item_idx: int, required
            The index of the item to be rated by the user.

        clipping: bool, default: True
            Whether to clip the predicted rating value.

        Returns
        -------
        A scalar
            A rating score of the user for the item
        """
        try:
            rating_pred = self.score(user_idx, item_idx)
        except ScoreException:
            rating_pred = self.default_score()

        if clipping:
            rating_pred = clip(rating_pred, self.min_rating, self.max_rating)

        return rating_pred

    def rank(self, user_idx, item_indices=None, k=-1, **kwargs):
        """Rank all test items for a given user.

        Parameters
        ----------
        user_idx: int, required
            The index of the user for whom to perform item raking.

        item_indices: 1d array, optional, default: None
            A list of candidate item indices to be ranked by the user.
            If `None`, list of ranked known item indices and their scores will be returned.

        k: int, required
            Cut-off length for recommendations, k=-1 will return ranked list of all items.
            This is more important for ANN to know the limit to avoid exhaustive ranking.

        Returns
        -------
        (ranked_items, item_scores): tuple
            `ranked_items` contains item indices being ranked by their scores.
            `item_scores` contains scores of items corresponding to index in `item_indices` input.

        """
        # obtain item scores from the model
        try:
            print('in try')
            known_item_scores = self.score(user_idx, **kwargs)
        except ScoreException:
            print('in catch')
            known_item_scores = np.ones(self.total_items) * self.default_score()

        # check if the returned scores also cover unknown items
        # if not, all unknown items will be given the MIN score
        if len(known_item_scores) == self.total_items:
            all_item_scores = known_item_scores
        else:
            all_item_scores = np.ones(self.total_items) * np.min(known_item_scores)
            all_item_scores[: self.num_items] = known_item_scores

        # rank items based on their scores
        item_indices = (
            np.arange(self.num_items)
            if item_indices is None
            else np.asarray(item_indices)
        )
        item_scores = all_item_scores[item_indices]
        if (
            k != -1
        ):  # O(n + k log k), faster for small k which is usually the case
            partitioned_idx = np.argpartition(item_scores, -k)
            top_k_idx = partitioned_idx[-k:]
            sorted_top_k_idx = top_k_idx[np.argsort(item_scores[top_k_idx])]
            partitioned_idx[-k:] = sorted_top_k_idx
            ranked_items = item_indices[partitioned_idx[::-1]]
            score_items_sorted = item_scores[partitioned_idx[::-1]]
        else:  # O(n log n)
            ranked_items = item_indices[item_scores.argsort()[::-1]]
#             score_items_sorted = item_scores[item_scores.argsort()[::-1]]

        return ranked_items, score_items_sorted

    def recommend(self, user_id, k=-1, remove_seen=False, train_set=None):
        """Generate top-K item recommendations for a given user. Key difference between
        this function and rank() function is that rank() function works with mapped
        user/item index while this function works with original user/item ID. This helps
        hide the abstraction of ID-index mapping, and make model usage and deployment cleaner.

        Parameters
        ----------
        user_id: str, required
            The original ID of the user.

        k: int, optional, default=-1
            Cut-off length for recommendations, k=-1 will return ranked list of all items.

        remove_seen: bool, optional, default: False
            Remove seen/known items during training and validation from output recommendations.

        train_set: :obj:`cornac.data.Dataset`, optional, default: None
            Training dataset needs to be provided in order to remove seen items.

        Returns
        -------
        recommendations: list
            Recommended items in the form of their original IDs.
        """
        user_idx = self.uid_map.get(user_id, -1)
        if user_idx == -1:
            raise ValueError(f"{user_id} is unknown to the model.")

        if k < -1 or k > self.total_items:
            raise ValueError(f"k={k} is invalid, there are {self.total_users} users in total.")

        item_indices = np.arange(self.total_items)
        if remove_seen:
            seen_mask = np.zeros(len(item_indices), dtype="bool")
            if train_set is None:
                raise ValueError("train_set must be provided to remove seen items.")
            if user_idx < train_set.csr_matrix.shape[0]:
                seen_mask[train_set.csr_matrix.getrow(user_idx).indices] = True
                item_indices = item_indices[~seen_mask]

        item_rank, score_items_sorted = self.rank(user_idx, item_indices,k)
        if k != -1:
            item_rank = item_rank[:k]
            score_items_sorted = score_items_sorted[:k]

        recommendations = [self.item_ids[i] for i in item_rank]
        return recommendations, score_items_sorted

    def monitor_value(self, train_set, val_set):
        """Calculating monitored value used for early stopping on validation set (`val_set`).
        This function will be called by `early_stop()` function.
        Note: `val_set` could be `None` thus it needs to be checked before usage.

        Parameters
        ----------
        train_set: :obj:`cornac.data.Dataset`, required
            User-Item preference data as well as additional modalities.

        val_set: :obj:`cornac.data.Dataset`, optional, default: None
            User-Item preference data for model selection purposes (e.g., early stopping).

        Returns
        -------
        :raise NotImplementedError
        """
        raise NotImplementedError()

    def early_stop(self, train_set, val_set, min_delta=0.0, patience=0):
        """Check if training should be stopped when validation loss has stopped improving.

        Parameters
        ----------
        train_set: :obj:`cornac.data.Dataset`, required
            User-Item preference data as well as additional modalities.

        val_set: :obj:`cornac.data.Dataset`, optional, default: None
            User-Item preference data for model selection purposes (e.g., early stopping).

        min_delta: float, optional, default: 0.
            The minimum increase in monitored value on validation set to be considered as improvement,
            i.e. an increment of less than `min_delta` will count as no improvement.

        patience: int, optional, default: 0
            Number of epochs with no improvement after which training should be stopped.

        Returns
        -------
        res : bool
            Return `True` if model training should be stopped (no improvement on validation set),
            otherwise return `False`.
        """
        self.current_epoch += 1
        current_value = self.monitor_value(train_set, val_set)
        if current_value is None:
            return False

        if np.greater_equal(current_value - self.best_value, min_delta):
            self.best_value = current_value
            self.best_epoch = self.current_epoch
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= patience:
                self.stopped_epoch = self.current_epoch

        if self.stopped_epoch > 0:
            print("Early stopping:")
            print("- best epoch = {}, stopped epoch = {}".format(self.best_epoch, self.stopped_epoch))
            print(
                "- best monitored value = {:.6f} (delta = {:.6f})".format(
                    self.best_value, current_value - self.best_value
                )
            )
            return True
        return False

import tensorflow.compat.v1 as tf
def conv_layer(
    input,
    num_input_channels,
    filter_height,
    filter_width,
    num_filters,
    seed=None,
    use_pooling=True,
):
    shape = [filter_height, filter_width, num_input_channels, num_filters]
    weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05, seed=seed))
    biases = tf.Variable(tf.constant(0.05, shape=[num_filters]))
    layer = tf.nn.conv2d(
        input=input, filter=weights, strides=[1, 1, 1, 1], padding="VALID"
    )
    layer = layer + biases
    if use_pooling:
        layer = tf.nn.max_pool(
            value=layer,
            ksize=[1, input.shape[1] - filter_height + 1, 1, 1],
            strides=[1, 1, 1, 1],
            padding="VALID",
        )
    layer = tf.nn.relu(layer)
    return layer, weights


def flatten_layer(layer):
    layer_shape = layer.get_shape()
    num_feature = layer_shape[1:4].num_elements()
    layer_flat = tf.reshape(layer, [-1, num_feature])
    return layer_flat, num_feature


def fc_layer(input, num_input, num_output, seed=None):
    weights = tf.Variable(
        tf.truncated_normal([num_input, num_output], stddev=0.05, seed=seed)
    )
    biases = tf.Variable(tf.constant(0.05, shape=[num_output]))
    layer = tf.matmul(input, weights) + biases
    layer = tf.nn.tanh(layer)
    return layer
class CNN_module:
    def __init__(
        self,
        output_dimension,
        dropout_rate,
        emb_dim,
        max_len,
        filter_sizes,
        num_filters,
        hidden_dim,
        seed,
        init_W,
        learning_rate=0.001,
    ):
        self.drop_rate = dropout_rate
        self.max_len = max_len
        self.seed = seed
        self.learning_rate = learning_rate
        self.init_W = tf.constant(init_W)
        self.output_dimension = output_dimension
        self.emb_dim = emb_dim
        self.filter_lengths = filter_sizes
        self.nb_filters = num_filters
        self.vanila_dimension = hidden_dim

        self._build_graph()

    def _build_graph(self):
        # create Graph
        self.model_input = tf.placeholder(dtype=tf.int32, shape=(None, self.max_len))
        self.v = tf.placeholder(dtype=tf.float32, shape=(None, self.output_dimension))
        self.sample_weight = tf.placeholder(dtype=tf.float32, shape=(None,))
        self.embedding_weight = tf.Variable(initial_value=self.init_W)

        self.seq_emb = tf.nn.embedding_lookup(self.embedding_weight, self.model_input)
        self.reshape = tf.reshape(self.seq_emb, [-1, self.max_len, self.emb_dim, 1])
        self.convs = []

        # Convolutional layers
        for i in self.filter_lengths:
            convolutional_layer, weights = conv_layer(
                input=self.reshape,
                num_input_channels=1,
                filter_height=i,
                filter_width=self.emb_dim,
                num_filters=self.nb_filters,
                use_pooling=True,
            )

            flat_layer, _ = flatten_layer(convolutional_layer)
            self.convs.append(flat_layer)

        self.model_output = tf.concat(self.convs, axis=-1)
        # Fully-connected layers
        self.model_output = fc_layer(
            input=self.model_output,
            num_input=self.model_output.get_shape()[-1],
            num_output=self.vanila_dimension,
        )
        # Dropout layer
        self.model_output = tf.nn.dropout(self.model_output, self.drop_rate)
        # Output layer
        self.model_output = fc_layer(
            input=self.model_output,
            num_input=self.vanila_dimension,
            num_output=self.output_dimension,
        )
        # Weighted MEA loss function
        self.mean_square_loss = tf.losses.mean_squared_error(
            labels=self.v,
            predictions=self.model_output,
            reduction=tf.losses.Reduction.NONE,
        )
        self.weighted_loss = tf.reduce_sum(
            tf.reduce_sum(self.mean_square_loss, axis=1, keepdims=True)
            * self.sample_weight
        )
        # RMSPro optimizer
        self.optimizer = tf.train.RMSPropOptimizer(
            learning_rate=self.learning_rate
        ).minimize(self.weighted_loss)



class ConvMF(Recommender):
    """
    Parameters
    ----------
    k: int, optional, default: 50
        The dimension of the user and item latent factors.

    n_epochs: int, optional, default: 50
        Maximum number of epochs for training.

    cnn_epochs: int, optional, default: 5
        Number of epochs for optimizing the CNN for each overall training epoch.

    cnn_bs: int, optional, default: 128
        Batch size for optimizing CNN.

    cnn_lr: float, optional, default: 0.001
        Learning rate for optimizing CNN.

    lambda_u: float, optional, default: 1.0
        The regularization hyper-parameter for user latent factor.

    lambda_v: float, optional, default: 100.0
        The regularization hyper-parameter for item latent factor.

    emb_dim: int, optional, default: 200
        The embedding size of each word. One word corresponds with [1 x emb_dim] vector in the embedding space

    max_len: int, optional, default 300
        The maximum length of item's document

    filter_sizes: list, optional, default: [3, 4, 5]
        The length of filters in convolutional layer

    num_filters: int, optional, default: 100
        The number of filters in convolutional layer

    hidden_dim: int, optional, default: 200
        The dimension of hidden layer after the pooling of all convolutional layers

    dropout_rate: float, optional, default: 0.2
        Dropout rate while training CNN

    give_item_weight: boolean, optional, default: True
        When True, each item will be weighted base on the number of user who have rated this item

    init_params: dict, optional, default: {'U':None, 'V':None, 'W': None}
        Initial U and V matrix and initial weight for embedding layer W

    trainable: boolean, optional, default: True
        When False, the model is not trained and Cornac assumes that the model already \
        pre-trained (U and V are not None).

    References
    ----------
    * Donghyun Kim1, Chanyoung Park1. ConvMF: Convolutional Matrix Factorization for Document Context-Aware Recommendation. \
    In :10th ACM Conference on Recommender Systems Pages 233-240
    """

    def __init__(
        self,
        name="ConvMF",
        k=50,
        n_epochs=50,
        cnn_epochs=5,
        cnn_bs=128,
        cnn_lr=0.001,
        lambda_u=1,
        lambda_v=100,
        emb_dim=200,
        max_len=300,
        filter_sizes=[3, 4, 5],
        num_filters=100,
        hidden_dim=200,
        dropout_rate=0.2,
        give_item_weight=True,
        trainable=True,
        verbose=False,
        init_params=None,
        seed=None,
    ):
        super().__init__(name=name, trainable=trainable, verbose=verbose)
        self.give_item_weight = give_item_weight
        self.n_epochs = n_epochs
        self.cnn_bs = cnn_bs
        self.cnn_lr = cnn_lr
        self.lambda_u = lambda_u
        self.lambda_v = lambda_v
        self.k = k
        self.dropout_rate = dropout_rate
        self.emb_dim = emb_dim
        self.max_len = max_len
        self.filter_sizes = filter_sizes
        self.num_filters = num_filters
        self.hidden_dim = hidden_dim
        self.name = name
        self.verbose = verbose
        self.cnn_epochs = cnn_epochs
        self.seed = seed

        # Init params if provided
        self.init_params = {} if init_params is None else init_params
        self.U = self.init_params.get("U", None)
        self.V = self.init_params.get("V", None)
        self.W = self.init_params.get("W", None)

    def _init(self, train_set):
        rng = get_rng(self.seed)
        n_users, n_items = train_set.num_users, train_set.num_items
        vocab_size = train_set.item_text.vocab.size

        if self.U is None:
            self.U = xavier_uniform((n_users, self.k), rng)
        if self.V is None:
            self.V = xavier_uniform((n_items, self.k), rng)
        if self.W is None:
            self.W = xavier_uniform((vocab_size, self.emb_dim), rng)

    def fit(self, train_set, val_set=None):
        """Fit the model to observations.

        Parameters
        ----------
        train_set: :obj:`cornac.data.Dataset`, required
            User-Item preference data as well as additional modalities.

        val_set: :obj:`cornac.data.Dataset`, optional, default: None
            User-Item preference data for model selection purposes (e.g., early stopping).

        Returns
        -------
        self : object
        """
        Recommender.fit(self, train_set, val_set)

        self._init(train_set)

        if self.trainable:
            self._fit_convmf(train_set)

        return self

    @staticmethod
    def _build_data(csr_mat):
        data = []
        index_list = []
        rating_list = []
        for i in range(csr_mat.shape[0]):
            j, k = csr_mat.indptr[i], csr_mat.indptr[i + 1]
            index_list.append(csr_mat.indices[j:k])
            rating_list.append(csr_mat.data[j:k])
        data.append(index_list)
        data.append(rating_list)
        return data

    def _fit_convmf(self, train_set):
        user_data = self._build_data(train_set.matrix)
        item_data = self._build_data(train_set.matrix.T.tocsr())

        n_user = len(user_data[0])
        n_item = len(item_data[0])

        # R_user and R_item contain rating values
        R_user = user_data[1]
        R_item = item_data[1]

        if self.give_item_weight:
            item_weight = np.array([math.sqrt(len(i)) for i in R_item], dtype=float)
            item_weight = (float(n_item) / item_weight.sum()) * item_weight
        else:
            item_weight = np.ones(n_item, dtype=float)

        # Initialize cnn module
        import tensorflow.compat.v1 as tf
        # from .convmf import CNN_module

        tf.disable_eager_execution()

        # less verbose TF
        os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
        tf.logging.set_verbosity(tf.logging.ERROR)

        tf.set_random_seed(self.seed)
        cnn_module = CNN_module(
            output_dimension=self.k,
            dropout_rate=self.dropout_rate,
            emb_dim=self.emb_dim,
            max_len=self.max_len,
            filter_sizes=self.filter_sizes,
            num_filters=self.num_filters,
            hidden_dim=self.hidden_dim,
            seed=self.seed,
            init_W=self.W,
            learning_rate=self.cnn_lr,
        )

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.Session(config=config)

        sess.run(tf.global_variables_initializer())  # init variable

        document = train_set.item_text.batch_seq(
            np.arange(n_item), max_length=self.max_len
        )

        feed_dict = {cnn_module.model_input: document}
        theta = sess.run(cnn_module.model_output, feed_dict=feed_dict)

        endure = 3
        converge_threshold = 0.01
        history = 1e-50
        loss = 0

        for epoch in range(1, self.n_epochs + 1):
            if self.verbose:
                print("Epoch: {}".format(epoch))
                logging.info("Epoch: {}".format(epoch))

            tic = time.time()

            user_loss = 0.0
            for i in range(n_user):
                idx_item = user_data[0][i]
                V_i = self.V[idx_item]
                R_i = R_user[i]

                A = self.lambda_u * np.eye(self.k) + V_i.T.dot(V_i)
                B = (V_i * (np.tile(R_i, (self.k, 1)).T)).sum(0)
                self.U[i] = np.linalg.solve(A, B)

                user_loss += self.lambda_u * np.dot(self.U[i], self.U[i])

            item_loss = 0.0
            for j in range(n_item):
                idx_user = item_data[0][j]
                U_j = self.U[idx_user]
                R_j = R_item[j]

                A = self.lambda_v * item_weight[j] * np.eye(self.k) + U_j.T.dot(U_j)
                B = (U_j * (np.tile(R_j, (self.k, 1)).T)).sum(
                    0
                ) + self.lambda_v * item_weight[j] * theta[j]
                self.V[j] = np.linalg.solve(A, B)

                item_loss += np.square(R_j - U_j.dot(self.V[j])).sum()

            loop = trange(
                self.cnn_epochs, desc="Optimizing CNN", disable=not self.verbose
            )
            for _ in loop:
                for batch_ids in train_set.item_iter(
                    batch_size=self.cnn_bs, shuffle=True
                ):
                    batch_seq = train_set.item_text.batch_seq(
                        batch_ids, max_length=self.max_len
                    )
                    feed_dict = {
                        cnn_module.model_input: batch_seq,
                        cnn_module.v: self.V[batch_ids],
                        cnn_module.sample_weight: item_weight[batch_ids],
                    }

                    sess.run([cnn_module.optimizer], feed_dict=feed_dict)

            feed_dict = {
                cnn_module.model_input: document,
                cnn_module.v: self.V,
                cnn_module.sample_weight: item_weight,
            }
            theta, cnn_loss = sess.run(
                [cnn_module.model_output, cnn_module.weighted_loss], feed_dict=feed_dict
            )

            loss = 0.5 * (user_loss + item_loss + self.lambda_v * cnn_loss)

            toc = time.time()
            elapsed = toc - tic
            converge = abs((loss - history) / history)

            if self.verbose:
                print(
                    "Loss: %.5f Elapsed: %.4fs Converge: %.6f "% (loss, elapsed, converge)
                )
                logging.info("Loss: %.5f Elapsed: %.4fs Converge: %.6f "% (loss, elapsed, converge))

            history = loss
            if converge < converge_threshold:
                endure -= 1
                if endure == 0:
                    break

        tf.reset_default_graph()

    def score(self, user_idx, item_idx=None):
        """Predict the scores/ratings of a user for an item.

        Parameters
        ----------
        user_idx: int, required
            The index of the user for whom to perform score prediction.

        item_idx: int, optional, default: None
            The index of the item for which to perform score prediction.
            If None, scores for all known items will be returned.

        Returns
        -------
        res : A scalar or a Numpy array
            Relative scores that the user gives to the item or to all known items

        """
        # print('score in convmf class ')
        if self.is_unknown_user(user_idx):
            raise ScoreException("Can't make score prediction for user %d" % user_idx)

        if item_idx is not None and self.is_unknown_item(item_idx):
            raise ScoreException("Can't make score prediction for item %d" % item_idx)

        if item_idx is None:
            return self.V.dot(self.U[user_idx, :])

        return self.V[item_idx, :].dot(self.U[user_idx, :])

    def get_vector_measure(self):
        """Getting a valid choice of vector measurement in ANNMixin._measures.

        Returns
        -------
        measure: MEASURE_DOT
            Dot product aka. inner product
        """
        return MEASURE_DOT

    def get_user_vectors(self):
        """Getting a matrix of user vectors serving as query for ANN search.

        Returns
        -------
        out: numpy.array
            Matrix of user vectors for all users available in the model.
        """
        return self.U

    def get_item_vectors(self):
        """Getting a matrix of item vectors used for building the index for ANN search.

        Returns
        -------
        out: numpy.array
            Matrix of item vectors for all items available in the model.
        """
        return self.V

NUM_FMT = "{:.4f}"
from collections import OrderedDict

def _table_format(data, headers=None, index=None, extra_spaces=0, h_bars=None):
    if headers is not None:
        data.insert(0, headers)
    if index is not None:
        index.insert(0, "")
        for idx, row in zip(index, data):
            row.insert(0, idx)

    column_widths = np.asarray([[len(str(v)) for v in row] for row in data]).max(axis=0)

    row_fmt = (
        " | ".join(["{:>%d}" % (w + extra_spaces) for w in column_widths][1:]) + "\n"
    )
    if index is not None:
        row_fmt = "{:<%d} | " % (column_widths[0] + extra_spaces) + row_fmt

    output = ""
    for i, row in enumerate(data):
        if h_bars is not None and i in h_bars:
            output += row_fmt.format(
                *["-" * (w + extra_spaces) for w in column_widths]
            ).replace("|", "+")
        output += row_fmt.format(*row)
    return output


class ExperimentResult(list):
    """
    Result Class for an Experiment. A list of :obj:`cornac.experiment.Result`.
    """

    def __str__(self):
        headers = list(self[0].metric_avg_results.keys())
        data, index = [], []
        for r in self:
            data.append([NUM_FMT.format(r.metric_avg_results[m]) for m in headers])
            index.append(r.model_name)
        return _table_format(data, headers, index, h_bars=[1])


class CVExperimentResult(ExperimentResult):
    """
    Result Class for a cross-validation Experiment.
    """

    def __str__(self):
        return "\n".join([r.__str__() for r in self])

class CrossValidation(BaseMethod):
    """Cross Validation Evaluation Method.

    Parameters
    ----------
    data: array-like, required
        Raw preference data in the triplet format [(user_id, item_id, rating_value)].

    n_folds: int, optional, default: 5
        The number of folds for cross validation.

    rating_threshold: float, optional, default: 1.0
        Threshold used to binarize rating values into positive or negative feedback for
        model evaluation using ranking metrics (rating metrics are not affected).

    partition: array-like, shape (n_observed_ratings,), optional, default: None
        The partition of ratings into n_folds (fold label of each rating) \
        If `None`, random partitioning is performed to assign each rating into a fold.

    seed: int, optional, default: None
        Random seed for reproducibility.

    exclude_unknowns: bool, optional, default: True
        If `True`, unknown users and items will be ignored during model evaluation.

    verbose: bool, optional, default: False
        Output running log.
    """

    def __init__(
        self,
        data,
        n_folds=5,
        rating_threshold=1.0,
        partition=None,
        seed=None,
        exclude_unknowns=True,
        verbose=False,
        **kwargs
    ):
        BaseMethod.__init__(
            self,
            data=data,
            rating_threshold=rating_threshold,
            seed=seed,
            exclude_unknowns=exclude_unknowns,
            verbose=verbose,
            **kwargs
        )

        self.n_folds = n_folds
        self.n_ratings = len(self.data)
        self.current_fold = 0
        self.current_split = None

        self._partition = self._validate_partition(partition)

    def _partition_data(self):
        """Partition ratings into n_folds"""
        fold_size = int(self.n_ratings / self.n_folds)
        remain_size = self.n_ratings - fold_size * self.n_folds

        partition = np.repeat(np.arange(self.n_folds), fold_size)
        self.rng.shuffle(partition)

        if remain_size > 0:
            remain_partition = self.rng.choice(
                self.n_folds, size=remain_size, replace=True, p=None
            )
            partition = np.concatenate((partition, remain_partition))

        return partition

    def _validate_partition(self, partition):
        if partition is None:
            return self._partition_data()
        elif len(partition) != self.n_ratings:
            raise ValueError(
                "The partition length must be equal to the number of ratings"
            )
        elif len(set(partition)) != self.n_folds:
            raise ValueError(
                "Number of folds in given partition different from %s" % (self.n_folds)
            )

        return partition

    def _get_train_test(self):
        if self.verbose:
            print("Fold: {}".format(self.current_fold + 1))

        test_idx = np.where(self._partition == self.current_fold)[0]
        train_idx = np.where(self._partition != self.current_fold)[0]

        train_data = safe_indexing(self.data, train_idx)
        test_data = safe_indexing(self.data, test_idx)
        self.build(train_data=train_data, test_data=test_data, val_data=test_data)

    def _next_fold(self):
        if self.current_fold < self.n_folds - 1:
            self.current_fold = self.current_fold + 1
        else:
            self.current_fold = 0

    def evaluate(self, model, metrics, user_based, show_validation):
        result = CVResult(model.name)

        for _ in range(self.n_folds):
            self._get_train_test()
            new_model = model.clone()  # clone a completely new model
            fold_result, _ = BaseMethod.evaluate(
                self, new_model, metrics, user_based, show_validation=False
            )
            result.append(fold_result)
            self._next_fold()

        result.organize()

        return result, None  # no validation result of CV

def ranking_eval(
    model,
    metrics,
    train_set,
    test_set,
    val_set=None,
    rating_threshold=1.0,
    exclude_unknowns=True,
    verbose=False,
    props=None,
):
    """Evaluate model on provided ranking metrics.

    Parameters
    ----------
    model: :obj:`cornac.models.Recommender`, required
        Recommender model to be evaluated.

    metrics: :obj:`iterable`, required
        List of rating metrics :obj:`cornac.metrics.RankingMetric`.

    train_set: :obj:`cornac.data.Dataset`, required
        Dataset to be used for model training. This will be used to exclude
        observations already appeared during training.

    test_set: :obj:`cornac.data.Dataset`, required
        Dataset to be used for evaluation.

    val_set: :obj:`cornac.data.Dataset`, optional, default: None
        Dataset to be used for model selection. This will be used to exclude
        observations already appeared during validation.

    rating_threshold: float, optional, default: 1.0
        The threshold to convert ratings into positive or negative feedback.

    exclude_unknowns: bool, optional, default: True
        Ignore unknown users and items during evaluation.

    verbose: bool, optional, default: False
        Output evaluation progress.

    props: dictionary, optional, default: None
        items propensity scores

    Returns
    -------
    res: (List, List)
        Tuple of two lists:
         - average result for each of the metrics
         - average result per user for each of the metrics
    """

    if len(metrics) == 0:
        return [], []

    avg_results = []
    user_results = [{} for _ in enumerate(metrics)]

    gt_mat = test_set.csr_matrix
    train_mat = train_set.csr_matrix
    val_mat = None if val_set is None else val_set.csr_matrix

    def pos_items(csr_row):
        return [
            item_idx
            for (item_idx, rating) in zip(csr_row.indices, csr_row.data)
            if rating >= rating_threshold
        ]

    test_user_indices = set(test_set.uir_tuple[0])
    for user_idx in tqdm.tqdm(test_user_indices, disable=not verbose, miniters=100):
        test_pos_items = pos_items(gt_mat.getrow(user_idx))
        if len(test_pos_items) == 0:
            continue

        u_gt_pos = np.zeros(test_set.num_items, dtype="float")
        u_gt_pos[test_pos_items] = 1

        val_pos_items = [] if val_mat is None else pos_items(val_mat.getrow(user_idx))
        train_pos_items = (
            []
            if train_set.is_unk_user(user_idx)
            else pos_items(train_mat.getrow(user_idx))
        )

        u_gt_neg = np.ones(test_set.num_items, dtype="int")
        u_gt_neg[test_pos_items + val_pos_items + train_pos_items] = 0

        item_indices = None if exclude_unknowns else np.arange(test_set.num_items)
        item_rank, item_scores = model.rank(user_idx, item_indices)

        total_pi = 0.0
        if props is not None:
            for idx, e in enumerate(u_gt_pos):
                if e > 0 and props[str(idx)] > 0:
                    u_gt_pos[idx] /= props[str(idx)]
                    total_pi += 1 / props[str(idx)]

        for i, mt in enumerate(metrics):
            mt_score = mt.compute(
                gt_pos=u_gt_pos,
                gt_neg=u_gt_neg,
                pd_rank=item_rank,
                pd_scores=item_scores,
            )

            user_results[i][user_idx] = mt_score

    # avg results of ranking metrics
    for i, mt in enumerate(metrics):
        avg_results.append(sum(user_results[i].values()) / len(user_results[i]))

    return avg_results, user_results


class PropensityStratifiedEvaluation(BaseMethod):
    """Propensity-based Stratified Evaluation Method proposed by Jadidinejad et al. (2021)

    Parameters
    ----------
    data: array-like, required
        Raw preference data in the triplet format [(user_id, item_id, rating_value)].

    test_size: float, optional, default: 0.2
        The proportion of the test set,
        if > 1 then it is treated as the size of the test set.

    val_size: float, optional, default: 0.0
        The proportion of the validation set, \
        if > 1 then it is treated as the size of the validation set.

    n_strata: int, optional, default: 2
        The number of strata for propensity-based stratification.

    rating_threshold: float, optional, default: 1.0
        Threshold used to binarize rating values into positive or negative feedback for
        model evaluation using ranking metrics (rating metrics are not affected).

    seed: int, optional, default: None
        Random seed for reproducibility.

    exclude_unknowns: bool, optional, default: True
        If `True`, unknown users and items will be ignored during model evaluation.

    verbose: bool, optional, default: False
        Output running log.

    References
    ----------
    Amir H. Jadidinejad, Craig Macdonald and Iadh Ounis,
    The Simpson's Paradox in the Offline Evaluation of Recommendation Systems,
    ACM Transactions on Information Systems (to appear)
    https://arxiv.org/abs/2104.08912
    """

    def __init__(
        self,
        data,
        test_size=0.2,
        val_size=0.0,
        n_strata=2,
        rating_threshold=1.0,
        seed=None,
        exclude_unknowns=True,
        verbose=False,
        **kwargs,
    ):
        BaseMethod.__init__(
            self,
            data=data,
            rating_threshold=rating_threshold,
            seed=seed,
            exclude_unknowns=exclude_unknowns,
            verbose=verbose,
            **kwargs,
        )

        self.n_strata = n_strata

        # estimate propensities
        self.props = self._estimate_propensities()

        # split the data into train/valid/test sets
        self.train_size, self.val_size, self.test_size = RatioSplit.validate_size(
            val_size, test_size, data
        )
        self._split()

    def _eval(self, model, test_set, val_set, user_based, props=None):
        metric_avg_results = OrderedDict()
        metric_user_results = OrderedDict()

        avg_results, user_results = rating_eval(
            model=model,
            metrics=self.rating_metrics,
            test_set=test_set,
            user_based=user_based,
        )
        for i, mt in enumerate(self.rating_metrics):
            metric_avg_results[mt.name] = avg_results[i]
            metric_user_results[mt.name] = user_results[i]

        avg_results, user_results = ranking_eval(
            model=model,
            metrics=self.ranking_metrics,
            train_set=self.train_set,
            test_set=test_set,
            val_set=val_set,
            rating_threshold=self.rating_threshold,
            exclude_unknowns=self.exclude_unknowns,
            verbose=self.verbose,
            props=props,
        )
        for i, mt in enumerate(self.ranking_metrics):
            metric_avg_results[mt.name] = avg_results[i]
            metric_user_results[mt.name] = user_results[i]

        return Result(model.name, metric_avg_results, metric_user_results)

    def _split(self):
        data_idx = self.rng.permutation(len(self.data))
        train_idx = data_idx[: self.train_size]
        test_idx = data_idx[-self.test_size :]
        val_idx = data_idx[self.train_size : -self.test_size]

        train_data = safe_indexing(self.data, train_idx)
        test_data = safe_indexing(self.data, test_idx)
        val_data = safe_indexing(self.data, val_idx) if len(val_idx) > 0 else None

        # build train/test/valid datasets
        self._build_datasets(
            train_data=train_data, test_data=test_data, val_data=val_data
        )

        # build stratified dataset
        self._build_stratified_dataset(test_data=test_data)

    def _estimate_propensities(self):
        # find the item's frequencies
        item_freq = defaultdict(int)
        for u, i, r in self.data:
            item_freq[i] += 1

        # fit the exponential param
        data = np.array([e for e in item_freq.values()], dtype="float")
        results = powerlaw.Fit(data, discrete=True, fit_method="Likelihood")
        alpha = results.power_law.alpha
        fmin = results.power_law.xmin

        if self.verbose:
            print("Powerlaw exponential estimates: %f, min=%d" % (alpha, fmin))

        # replace raw frequencies with the estimated propensities
        for k, v in item_freq.items():
            if v > fmin:
                item_freq[k] = pow(v, alpha)

        return item_freq  # user-independent propensity estimations

    def _build_stratified_dataset(self, test_data):
        # build stratified datasets
        self.stratified_sets = {}

        # match the corresponding propensity score for each feedback
        test_props = np.array([self.props[i] for u, i, r in test_data], dtype="float")

        # stratify
        minp = min(test_props) - 0.01 * min(test_props)
        maxp = max(test_props) + 0.01 * max(test_props)
        slice = (maxp - minp) / self.n_strata
        strata = [
            f"Q{idx}"
            for idx in np.digitize(x=test_props, bins=np.arange(minp, maxp, slice))
        ]

        for stratum in sorted(np.unique(strata)):
            # sample the corresponding sub-population
            qtest_data = []
            for (u, i, r), q in zip(test_data, strata):
                if q == stratum:
                    qtest_data.append((u, i, r))

            # build a dataset
            qtest_set = Dataset.build(
                data=qtest_data,
                fmt=self.fmt,
                global_uid_map=self.global_uid_map,
                global_iid_map=self.global_iid_map,
                seed=self.seed,
                exclude_unknowns=self.exclude_unknowns,
            )
            if self.verbose:
                print("---")
                print("Test data ({}):".format(stratum))
                print("Number of users = {}".format(len(qtest_set.uid_map)))
                print("Number of items = {}".format(len(qtest_set.iid_map)))
                print("Number of ratings = {}".format(qtest_set.num_ratings))
                print("Max rating = {:.1f}".format(qtest_set.max_rating))
                print("Min rating = {:.1f}".format(qtest_set.min_rating))
                print("Global mean = {:.1f}".format(qtest_set.global_mean))
                print(
                    "Number of unknown users = {}".format(
                        qtest_set.num_users - self.train_set.num_users
                    )
                )
                print(
                    "Number of unknown items = {}".format(
                        self.test_set.num_items - self.train_set.num_items
                    )
                )

            self.stratified_sets[stratum] = qtest_set

    def evaluate(self, model, metrics, user_based, show_validation=True):
        """Evaluate given models according to given metrics

        Parameters
        ----------
        model: :obj:`cornac.models.Recommender`
            Recommender model to be evaluated.

        metrics: :obj:`iterable`
            List of metrics.

        user_based: bool, required
            Evaluation strategy for the rating metrics. Whether results
            are averaging based on number of users or number of ratings.

        show_validation: bool, optional, default: True
            Whether to show the results on validation set (if exists).

        Returns
        -------
        res: :obj:`cornac.experiment.Result`
        """
        result = PSTResult(model.name)

        if self.train_set is None:
            raise ValueError("train_set is required but None!")
        if self.test_set is None:
            raise ValueError("test_set is required but None!")

        self._reset()
        self._organize_metrics(metrics)

        ###########
        # FITTING #
        ###########
        if self.verbose:
            print("\n[{}] Training started!".format(model.name))

        start = time.time()
        model.fit(self.train_set, self.val_set)
        train_time = time.time() - start

        ##############
        # EVALUATION #
        ##############
        if self.verbose:
            print("\n[{}] Evaluation started!".format(model.name))

        # evaluate on the sampled test set (closed-loop)
        test_result = self._eval(
            model=model,
            test_set=self.test_set,
            val_set=self.val_set,
            user_based=user_based,
        )
        test_result.metric_avg_results["SIZE"] = self.test_set.num_ratings
        result.append(test_result)

        if self.verbose:
            print("\n[{}] IPS Evaluation started!".format(model.name))

        # evaluate based on Inverse Propensity Scoring
        ips_result = self._eval(
            model=model,
            test_set=self.test_set,
            val_set=self.val_set,
            user_based=user_based,
            props=self.props,
        )
        ips_result.metric_avg_results["SIZE"] = self.test_set.num_ratings
        result.append(ips_result)

        if self.verbose:
            print("\n[{}] Stratified Evaluation started!".format(model.name))

        # evaluate on different strata
        start = time.time()

        for _, qtest_set in self.stratified_sets.items():
            qtest_result = self._eval(
                model=model,
                test_set=qtest_set,
                val_set=self.val_set,
                user_based=user_based,
            )

            test_time = time.time() - start
            qtest_result.metric_avg_results["SIZE"] = qtest_set.num_ratings

            result.append(qtest_result)

        result.organize()

        val_result = None
        if show_validation and self.val_set is not None:
            start = time.time()
            val_result = self._eval(
                model=model, test_set=self.val_set, val_set=None, user_based=user_based
            )
            val_time = time.time() - start

        return result, val_result

# Experiment
class Experiment:
    """ Experiment Class

    Parameters
    ----------
    eval_method: :obj:`<cornac.eval_methods.BaseMethod>`, required
        The evaluation method (e.g., RatioSplit).

    models: array of :obj:`<cornac.models.Recommender>`, required
        A collection of recommender models to evaluate, e.g., [C2PF, HPF, PMF].

    metrics: array of :obj:{`<cornac.metrics.RatingMetric>`, `<cornac.metrics.RankingMetric>`}, required
        A collection of metrics to use to evaluate the recommender models, \
        e.g., [NDCG, MRR, Recall].

    user_based: bool, optional, default: True
        This parameter is only useful if you are considering rating metrics. When True, first the average performance \
        for every user is computed, then the obtained values are averaged to return the final result.
        If `False`, results will be averaged over the number of ratings.

    show_validation: bool, optional, default: True
        Whether to show the results on validation set (if exists).

    verbose: bool, optional, default: False
        Output running log/progress during model training and evaluation.
        If verbose is True, it will overwrite verbosity setting of evaluation method and models.

    save_dir: str, optional, default: None
        Path to a directory for storing trained models and logs. If None,
        models will NOT be stored and logs will be saved in the current working directory.

    Attributes
    ----------
    result: array of :obj:`<cornac.experiment.result.Result>`, default: None
        This attribute contains the results per-model of your experiment
        on the test set, initially it is set to None.

    val_result: array of :obj:`<cornac.experiment.result.Result>`, default: None
        This attribute contains the results per-model of your experiment
        on the validation set (if exists), initially it is set to None.

    """

    def __init__(
        self,
        eval_method,
        models,
        metrics,
        user_based=True,
        show_validation=True,
        verbose=False,
        save_dir=None,
    ):
        self.eval_method = eval_method
        self.models = self._validate_models(models)
        self.metrics = self._validate_metrics(metrics)
        self.user_based = user_based
        self.show_validation = show_validation
        self.verbose = verbose
        self.save_dir = save_dir
        self.result = None
        self.val_result = None

    @staticmethod
    def _validate_models(input_models):
        if not hasattr(input_models, "__len__"):
            raise ValueError(
                "models have to be an array but {}".format(type(input_models))
            )

        valid_models = []
        for model in input_models:
            if isinstance(model, Recommender):
                valid_models.append(model)
        return valid_models

    @staticmethod
    def _validate_metrics(input_metrics):
        if not hasattr(input_metrics, "__len__"):
            raise ValueError(
                "metrics have to be an array but {}".format(type(input_metrics))
            )

        valid_metrics = []
        for metric in input_metrics:
            if isinstance(metric, RatingMetric) or isinstance(metric, RankingMetric):
                valid_metrics.append(metric)
        return valid_metrics

    def _create_result(self):
        # from ..eval_methods.cross_validation import CrossValidation
        # from ..eval_methods.propensity_stratified_evaluation import (
        #     PropensityStratifiedEvaluation,
        # )

        if isinstance(self.eval_method, CrossValidation) or isinstance(
            self.eval_method, PropensityStratifiedEvaluation
        ):
            self.result = CVExperimentResult()
        else:
            self.result = ExperimentResult()
            if self.show_validation and self.eval_method.val_set is not None:
                self.val_result = ExperimentResult()

    def run(self):
        """Run the Cornac experiment"""
        self._create_result()

        # overwrite verbosity setting of evaluation method and models
        # if Experiment verbose is True
        if self.verbose:
            self.eval_method.verbose = self.verbose
            for model in self.models:
                model.verbose = self.verbose

        for model in self.models:
            test_result, val_result = self.eval_method.evaluate(
                model=model,
                metrics=self.metrics,
                user_based=self.user_based,
                show_validation=self.show_validation,
            )

            self.result.append(test_result)
            if self.val_result is not None:
                self.val_result.append(val_result)

            if self.save_dir and (not isinstance(self.result, CVExperimentResult)):
                model.save(self.save_dir)

        output = ""
        if self.val_result is not None:
            output += "\nVALIDATION:\n...\n{}".format(self.val_result)
        output += "\nTEST:\n...\n{}".format(self.result)

        print(output)
        logging.info('Test dataset result: ')
        logging.info(output)

        # timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S-%f")
        # save_dir = "." if self.save_dir is None else self.save_dir
        # output_file = os.path.join(save_dir, "CornacExp-{}.log".format(timestamp))
        # with open(output_file, "w") as f:
        #     f.write(output)

def _filter(tuples,item_set, fmt="UIR"):
        i_pos = fmt.find("I")

        if item_set is not None:
            tuples = [t for t in tuples if t[i_pos] in item_set]

        return tuples

# run model
# from datasets import movielens
def load_data_vietnamese():
    with open('./books_new_2000.txt', encoding='utf-8', errors=None) as f:
        texts, movie_ids = [], []
        for line in f:
            tokens = line.strip().split("::")
            movie_ids.append(tokens[0])
            texts.append("::".join(tokens[1:]))

    return texts, movie_ids

# plots, movie_ids = load_plot()
# ml_1m = load_feedback(variant="100K", reader=Reader(item_set=movie_ids))

import itertools
def load_books_feedback(movie_ids):
    with open('./data.txt', encoding="utf-8", errors=None) as f:
        tuples = [
            tup
            for idx, line in enumerate(itertools.islice(f, 0, None))
            for tup in uir_parser(line.strip().split('\t'))
            ]
        tuples = _filter(tuples=tuples,item_set=movie_ids, fmt='UIR')

    return tuples

def train_model():
    plots, movie_ids = load_data_vietnamese()
    ml_1m = load_books_feedback(movie_ids)

    # Instantiate a TextModality, it makes it convenient to work with text auxiliary information
    # For more details, please refer to the tutorial on how to work with auxiliary data
    item_text_modality = TextModality(
        corpus=plots,
        ids=movie_ids,
        tokenizer=BaseTokenizer(sep="\t", stop_words="vietnamese"),
        max_vocab=8000,
        max_doc_freq=0.5,
    )

    # Define an evaluation method to split feedback into train and test sets
    ratio_split = RatioSplit(
        data=ml_1m,
        test_size=0.2,
        exclude_unknowns=True,
        item_text=item_text_modality,
        verbose=True,
        seed=123,
    )

    # Instantiate ConvMF model
    convmf = ConvMF(n_epochs=5, verbose=True, seed=123)

    # Instantiate RMSE for evaluation
    rmse = RMSE()

    # Put everything together into an experiment and run it
    Experiment(
        eval_method=ratio_split, models=[convmf], metrics=[rmse], user_based=True
    ).run()
    convmf.save(save_dir="./model_convmf")


def predict():
    try:
        model = ConvMF.load("./model_convmf/")
        # Obtain item recommendations for user U1
        recs,score_item = model.recommend(user_id="1", k=50)
        print(recs)
        print(score_item)
    except ValueError as e:
        # Handle the ValueError if the user_id is unknown
        print(f"Error: {e}")

def load_model():
    try:
        model = ConvMF.load("./model_convmf/")
        print('load model convmf successfully')
        return model
    except Exception as e:
        print('error: ', e)
        return None
    
